{"meta":{"title":"keon随便写写","subtitle":"大概是一些读书笔记","description":null,"author":"keon","url":"https://awdclijn.github.io"},"pages":[{"title":"about","date":"2020-06-06T09:31:43.000Z","updated":"2020-06-06T09:32:27.190Z","comments":true,"path":"about/index.html","permalink":"https://awdclijn.github.io/about/index.html","excerpt":"","text":"test"},{"title":"categories","date":"2018-07-14T05:58:22.000Z","updated":"2018-07-14T06:07:41.138Z","comments":false,"path":"categories/index.html","permalink":"https://awdclijn.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-07-14T05:59:56.000Z","updated":"2018-07-14T06:05:49.671Z","comments":false,"path":"tags/index.html","permalink":"https://awdclijn.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"《Flink基础教程》--读书笔记1","slug":"《Flink基础教程》-读书笔记1","date":"2018-12-05T11:34:53.000Z","updated":"2020-06-06T13:24:37.222Z","comments":true,"path":"2018/12/05/《Flink基础教程》-读书笔记1/","link":"","permalink":"https://awdclijn.github.io/2018/12/05/《Flink基础教程》-读书笔记1/","excerpt":"流处理的优势： 能够以非常低的延迟处理数据 较低的成本能带来较好的容错性 流式技术的发展","text":"流处理的优势： 能够以非常低的延迟处理数据 较低的成本能带来较好的容错性 流式技术的发展 Flink架构 Flink的分布式特点体现在它能够在成百上千台机器上运行，它将大型的计算任务分成许多小的部分，每个机器执行一个部分。Flink 能够自动地确保在发生机器故障或者其他错误时计算能持续进行，或者在修复bug或进行版本升级后有计划地再执行一次。这种能力使得开发人员不需要担心失败。Flink本质上使用容错性数据流，这使得开发人员可以分析持续生成且永远不结束的数据(即流处理)。 Flink常用的架构： 数据传输层1） 高性能及持久化：作为数据缓冲区，将短时间的数据保留，持久化的特性使得消息可以重播。2） 将生产者和消费者进行解耦： 可以支持从多个源收集数据，并把数据提供给多个服务使用。 流处理层 事件时间 是事件创建的时间。它通常由事件中的时间戳描述，例如附接在生产传感器，或者生产服务。Flink通过时间戳分配器访问事件时间戳。摄入时间 是事件进入Flink数据流源算符的时间。处理事件 是每一个执行时间操作的算符的本地时间。 窗口 时间窗口，有滚动窗口和滑动窗口，滚动按结束时间，滑动按开始时间。 stream.timeWindow(Time.minutes(1), Time.seconds(30)) 计数窗口， stream.countWindow(4, 2) 会话窗口，用户与网站进行一系列 交互(活动阶段)之后，关闭浏览器或者不再交互(非活动阶段) stream.window(SessionWindows.withGap(Time.minutes(5)) 水印表示当前不会再有比水印时间早的流水，具体见如下链接：https://blog.csdn.net/a6822342/article/details/78064815 状态流式计算分为无状态和有状态两种情况。无状态的计算观察每个独立事件，并根据最后一个事件输出结果。有状态的计算则会基于多个事件输出结果。 一致性 at-most-once:这其实是没有正确性保障的委婉说法——故障发生之后，计数结果可能丢失。 at-least-once:这表示计数结果可能大于正确值，但绝不会小于正确值。也就是说，计数程序在发生故障后可能多算，但是绝不会少算。 exactly-once:这指的是系统保证在发生故障后得到的计数结果与正确值一致。 容错检查点 Flink使用 流重放 与 Checkpoint 的结合实现了容错。Checkpoint与每一个输入流及其相关的每一个算符的状态的特定点相关联。一个流数据流可以可以从一个checkpoint恢复出来，其中通过恢复算符状态并从检查点重放事件以保持一致性（一次处理语义）检查点间隔是以恢复时间（需要重放的事件数量）来消除执行过程中容错的开销的一种手段。 Flink基础教程比较薄，对介绍的特性也是点到即止。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Flink基础教程","slug":"读书笔记/Flink基础教程","permalink":"https://awdclijn.github.io/categories/读书笔记/Flink基础教程/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"Flink","slug":"Flink","permalink":"https://awdclijn.github.io/tags/Flink/"}]},{"title":"论软件可靠性设计技术的应用","slug":"论软件可靠性设计技术的应用","date":"2018-11-09T02:14:49.000Z","updated":"2018-11-26T15:38:14.152Z","comments":true,"path":"2018/11/09/论软件可靠性设计技术的应用/","link":"","permalink":"https://awdclijn.github.io/2018/11/09/论软件可靠性设计技术的应用/","excerpt":"【摘要】2017年4月份，某股份制银行启动了新征信管理平台的系统项目，本人所在的大数据支持处室负责了数据存储、查询及数据标准化等功能的建设，本人作为项目经办负责技术方案的管理和设计。征信管理平台是我行风险控制的重要部分，维护有个人、对公征信、互联网小贷、电信运营商等数据。征信管理平台承载了大量业务在风险控制环节对数据的诉求，如新发卡、贷前、贷中等关键业务流程，因此行内提出了较高的可靠性要求。为达到该系统的可靠性要求，本人带领合作公司团队对系统的运行环境和特点等进行分析，根据分析出的结果制定了提高系统可靠性的措施：一是采用了健康检查的思路，出现故障后可自动拉起应用并告警；二是降低主流程复杂度，将主流程外的处理通过队列异步处理。三是容错机制的设计，建议业务采购多家供应商的接口，形成主备模式。至从2017年6月开放第一批接口，截至2017年年末已接入数据源40余个，系统投产后大量的外部征信数据能在有效期内得到复用，节约了大量的费用，同时系统的高性能、高可用性的特点也得到业务部门的一致好评。","text":"【摘要】2017年4月份，某股份制银行启动了新征信管理平台的系统项目，本人所在的大数据支持处室负责了数据存储、查询及数据标准化等功能的建设，本人作为项目经办负责技术方案的管理和设计。征信管理平台是我行风险控制的重要部分，维护有个人、对公征信、互联网小贷、电信运营商等数据。征信管理平台承载了大量业务在风险控制环节对数据的诉求，如新发卡、贷前、贷中等关键业务流程，因此行内提出了较高的可靠性要求。为达到该系统的可靠性要求，本人带领合作公司团队对系统的运行环境和特点等进行分析，根据分析出的结果制定了提高系统可靠性的措施：一是采用了健康检查的思路，出现故障后可自动拉起应用并告警；二是降低主流程复杂度，将主流程外的处理通过队列异步处理。三是容错机制的设计，建议业务采购多家供应商的接口，形成主备模式。至从2017年6月开放第一批接口，截至2017年年末已接入数据源40余个，系统投产后大量的外部征信数据能在有效期内得到复用，节约了大量的费用，同时系统的高性能、高可用性的特点也得到业务部门的一致好评。【正文】随着银行业的快速发展，各业务部门对风险管理的要求越来越高，需要更加丰富的征信数据帮忙完成业务流程，同时旧征信平台中使用的传统应用和数据库架构已无法满足业务发展的需求，因此行内提出了建设新征信管理平台的诉求。大量的征信外部数据也是对大数据平台数据源的一个完善，因此我所在的处室承担了相关数据存储、查询及数据标准化等功能的建设（本文中仅对实时应用部分讨论，离线部分不具体展开），本人负责技术方案的管理和设计。总体上系统采用了微服务的架构：1、查询服务按业务领域进行划分，有征信服务、运营商服务、个人属性服务等负责具体数据的查询、主备供应商的切换及数据的标准化，并且将查询后的数据及相关的日志信息防止队列中间件中；2、数据回写服务负责从队列将数据入库，并根据规则生成有效期等字段，以便数据复用。3、服务治理的相关组件，注册中心负责管理服务的元信息；配置中心负责管理各服务的配置文件；网关负责交易请求的路由。4、数据存储上采用了分布式的中间件Elasticsearch，过程中直接存取非结构化数据，减少关联查询。 征信管理平台涉及的关键业务流程多，访问量大，因此行内提出了较高的可靠性要求。要提高系统的可靠性指标，团队首先进行可靠性影响分析，主要采用了故障树分析方法。故障树分析方法是一种自顶向下的软件可靠性分析方法，从软件系统不希望发生的事件向下逐步追查导致事件发生的原因，直至基本事件，从而确定软件故障的原因。经过分析，我们找出了以下导致故障发生的基本事件： 一、运行硬件，应用所依赖的运行硬件并不是完全可靠的，随着时间推移，硬件会发生老化从而产生故障。 二、模块规模及内部结构的复杂度，模块的规模越大、复杂度越高，其开发测试的难度也越大，我们需要尽量降低主流程的复杂度，让整个架构实现更加可控。 三、外部依赖，征信数据的来源是第三方的数据服务提供商，每个服务提供商的服务质量参差不齐。虽然有商务合同进行约束，但实际情况中经常会出现无法提供服务的情况。 常用的软件可靠性技术有容错设计、检错设计、降低复杂度设计等。容错设计可以通过冗余或者备份的方式代替故障服务，从而维护软件系统的正常运行；检错设计是当软件系统出现故障时能及时发现并告警提示维护人员处理的能力；降低复杂度设计的思想是保证实现软件功能的基础上，简化软件结构，优化软件数据流向，降低软件复杂度，从而提高软件可靠性。根据以上的分析，结合行内的技术特点，我们制定了如下的可靠性设计方案。 一、在整体的部署架构中采用多活的方式进行部署，建立健康检查的机制，并且与行内集中告警平台对接。 应用服务以多实例的方式部署在行内已有docker云平台中，应用暴露接口供云平台进行健康检查，若长时间判断为不健康时，云平台可自动拉起新的服务实例，关闭故障实例，并进行统一告警。同时在应用逻辑中提供了重试的请求机制，通过网关服务进行流量分发时，若无法分发至目标服务，则会重试分发逻辑。如仍无法分发，则会从注册中心中挑选另一个可用实例进行分发路由，保障服务的连续。 二、降低业务主流程程序中的复杂度，将回写数据、日志信息等放入到kafka消息队列中间件中，后续由其他应用进行数据消费处理。 征信数据在业务定义的有效期内可反复使用，节约外发请求第三方的费用，因此需要将查询后的数据结果进行存储。在该方案设计中，查询后的数据会直接放入到消息队列中，由数据回写服务根据业务配置的有效期规则，计算数据有效期后进行入库。该设计将查询和写入操作解耦，模块之间消除依赖，查询服务外的应用出现故障，最坏结果只是会造成服务降级，无法查得存量数据，并不会影响正常的业务的开展。由于各服务的轻量级设计思路，后续的维护复杂度降低，便于评估变更影响，提高整个系统的可靠性。 三、采用了冗余设计的思路，向业务建议采购多个提供相同服务的数据提供商形成服务的主备架构。 第三方的服务质量及数据质量都无法控制，从设计上规划了两种策略进行冗余，1）主备供应商，即查询时根据业务配置的主供应商进行查询，业务可按需进行切换；2）优先级策略，即优先查询高优先级的供应商，若无法得到满足业务的服务（如第三方服务无响应或关键字段为空），则继续查询下一优先级的服务，保证业务不受第三方影响。这个设计思路也得到了业务的高度认可，在某次快速发卡（现场申请虚拟信用卡进行消费）的业务场景中，第三方接口突然服务中断，业务按指引进行了供应商优先级的切换，快速的解决了问题，保证了业务不受影响。 该项目架构稳定后，本人于17年年底交接了该系统的相关事项，截至当时，已接入数据源40余个，每日的查询量已接近千万级。在项目组各位的努力下，该系统也得到了各部门的一致好评。通过本次的项目架构实践，我学习了提高可靠性相关的分析方法和设计思路，也深刻体会到了系统架构设计和选择对项目影响的重要性。","categories":[],"tags":[]},{"title":"论软件系统架构评估","slug":"论软件系统架构评估","date":"2018-11-09T01:02:18.000Z","updated":"2018-11-26T15:43:04.171Z","comments":true,"path":"2018/11/09/论软件系统架构评估/","link":"","permalink":"https://awdclijn.github.io/2018/11/09/论软件系统架构评估/","excerpt":"2017年4月份，某股份制银行启动了新征信管理平台的系统项目，本人所在的大数据支持处室负责了数据存储、查询及数据标准化等功能的建设，本人作为项目经办负责技术方案的管理和设计。征信管理平台是我行风险控制的重要部分，维护有个人、对公征信、互联网小贷、电信运营商等数据。征信管理平台承载了大量业务在风险控制环节对数据的诉求，如新发卡、贷前、贷中等关键业务流程。本文论述了软件系统的架构评估，首先分析了软件架构评估所普遍关注的质量属性并阐述了其性能、可用性、可修改性和安全性的具体含义。整个系统采用了微服务的架构设计方法。在架构设计完成之后，对SA评估采用了基于场景的评估方式中的体系结构权衡分析方法ATAM，并详细描述了其评估过程，项目评估小组经过对项目的风险点、敏感点和权衡点的讨论后生成了质量效应树。目前系统已稳定运行一年多，从而验证了该项目采用ATAM架构评估保证了系统的顺利完成。","text":"2017年4月份，某股份制银行启动了新征信管理平台的系统项目，本人所在的大数据支持处室负责了数据存储、查询及数据标准化等功能的建设，本人作为项目经办负责技术方案的管理和设计。征信管理平台是我行风险控制的重要部分，维护有个人、对公征信、互联网小贷、电信运营商等数据。征信管理平台承载了大量业务在风险控制环节对数据的诉求，如新发卡、贷前、贷中等关键业务流程。本文论述了软件系统的架构评估，首先分析了软件架构评估所普遍关注的质量属性并阐述了其性能、可用性、可修改性和安全性的具体含义。整个系统采用了微服务的架构设计方法。在架构设计完成之后，对SA评估采用了基于场景的评估方式中的体系结构权衡分析方法ATAM，并详细描述了其评估过程，项目评估小组经过对项目的风险点、敏感点和权衡点的讨论后生成了质量效应树。目前系统已稳定运行一年多，从而验证了该项目采用ATAM架构评估保证了系统的顺利完成。 随着银行业的快速发展，各业务部对风险管理的要求越来越高，需要更加丰富的征信数据帮忙完成业务流程，同时旧征信平台中使用的传统应用和数据库架构已无法满足业务发展的需求，因此行内提出了建设新征信管理平台的诉求。大量的征信外部数据也是对大数据平台数据源的一个完善，因此我所在的处室承担了相关数据存储、查询及数据标准化等功能的建设（本文中仅对实时应用部分讨论，离线部分不具体展开），本人负责技术方案的管理和设计。 总体上系统采用了微服务的架构：1、查询服务按业务领域进行划分，有征信服务、运营商服务、个人属性服务等负责具体数据的查询、主备供应商的切换及数据的标准化，并且将查询后的数据及相关的日志信息防止队列中间件中；2、数据回写服务负责从队列将数据入库，并根据规则生成有效期等字段，以便数据复用。3、服务治理的相关组件，注册中心负责管理服务的元信息；配置中心负责管理各服务的配置文件；网关负责交易请求的路由。4、数据存储上采用了分布式的中间件Elasticsearch，过程中直接存取非结构化数据，减少关联查询。征信管理平台涉及的关键业务流程多，访问量大，因此行内提出了较多的质量属性要求，下面先介绍下软件架构评估的质量属性。 架构评估是软件开发过程中的重要环节，在软件架构评估中的质量属性有：性能、可用性、可修改性、安全性、可测试性、可靠性和易用性等。其中前4个质量属性是质量效应树的重要组成部分。性能是指系统的响应能力，即经过多长时间对事件做出响应。可用性是指系统能够正常运行的比例，通过用两次故障之间的时间长度或出现故障时系统能够恢复的速度来表示。可修改性是指系统能以较高的性价比对系统做出变更的能力。安全性是指系统能够向合法用户提供服务，同时拒绝非授权用户使用或拒绝服务的能力。常用的架构评估方法有：基于问卷调查的评估方式、基于场景的评估方式和基于度量的评估方式。基于问卷调查的评估方式是由多个评估专家通过调查问卷的方式回答问卷中的问题，对多个评估结果进行综合，最终得到最终结果。其评价的具有主观性不太适合本项目。基于度量的评估方式虽然评价比较客观，但是需要评估者对系统的架构有精确的了解，也不太适合本项目。而基于场景的评估要求评估者对系统中等了解，评价比较主观，故本项目采用了基于场景的评估方式。基于场景的评估方式又分为架构权衡分析法ATAM，软件架构分析法SAAM和成本效益分析法CBAM。本项目中根据不同质量属性使用了ATAM作为系统架构评估的方法。 在使用ATAM进行架构评估时，我们根据项目需要成立了项目评估小组。其主要成员包括：评估小组负责人、项目决策者、架构设计师、用户、开发人员、测试人员、系统部署人员等项目干系人。架构的评估经历了描述和介绍阶段、调查和分析阶段、测试阶段和报告阶段四个阶段。下面我分别从这四个阶段进行介绍。 在描述和介绍阶段，由于项目评估成员有部分人员对ATAM并不熟悉，因此项目组首先学习了ATAM的方法。它是一种基于场景的软件架构评估方法，对系统的多个质量属性基于场景进行评估。通过该评估确认系统存在的风险，并检查各自的非功能性需求是否满足需求。业务重点阐述了系统的目的和举例说明了使用的场景，我也和大家介绍了本次项目中采用的微服务架构，并解析了各模块的功能。 在调查分析阶段，业务部门提出了性能及高可用的要求，因为大量的业务场景需要使用到这些数据，并且部分场景对时间非常敏感。另外开发人员提出为了当不断增加数据供应商时，需要快速的进行接入，保证系统的开发效率及系统修改性，可以进行并行开发。 针对这些场景我们分析了项目开发过程中的风险点、敏感点和权衡点。经过分析，该项目中存在以下风险点：当第三方供应商无法提供服务时，会直接影响业务可用性；敏感点有：用户的加密级别、漏洞规则的修改。权衡点有：为提高可用性，需增加主备模式，当主供应商失效时，能切换至选供应商继续提供服务，但因为主备供应商的报文格式、数据字典都有差异，需要进行标准化，势必影响系统的可修改性。 在测试阶段：经过评估小组集体讨论，确定了不同场景的优先级如下：系统的可用性最高，性能其次，可修改性较低。在保证系统可用性方面，在流量捕获部分使用双机热备技术，在两个捕获系统之间设置心跳，当一台捕获系统出问题，另一台捕获设备接管。在流量自动化分析部分，采用了集群部署技术，一台分析设备出问题，不会影响整个分析系统。在保证数据安全性方面，磁盘采用企业磁盘阵列raid5机制。在用户数据安全性方面，采用了非对称加密及信息摘要技术。 最后形成了评估报告，经过对架构的评估，确定了系统的风险点、敏感点、权衡点和非风险点，最后以文档的形式表现。其包括的内容包括：架构分析方法文档、架构的不同场景及各自的优先级、质量效应树、风险点决策、非风险点决策及每次的评估会议记录。 该项目开发工作于2016年8月完工，系统上线后，我们的安全分析人员和客户使用该系统对互联网流量进行漏洞挖掘，一共产生了150种以上的web流量攻击流量特征和5个未知web漏洞。在国家某安全中心网研室的其他项目中起到了支撑作用，尤其是某变量覆盖漏洞、某文件写入漏洞，某sql注入漏洞在项目使用过程中取得了一定得效果，得到了好评。为开展互联网安全事件得防御、发现、预警和协调处置等工作提供了数据依据，更好的维护了国家公共互联网安全，保障基础信息网络和重要信息系统的安全运行。","categories":[],"tags":[]},{"title":"论软件架构的选择","slug":"论软件架构的选择","date":"2018-11-09T00:40:14.000Z","updated":"2018-11-26T15:38:10.852Z","comments":true,"path":"2018/11/09/论软件架构的选择/","link":"","permalink":"https://awdclijn.github.io/2018/11/09/论软件架构的选择/","excerpt":"论文摘要 2017年4月份，某股份制银行启动了新征信管理平台的系统项目，本人所在的大数据支持处室负责了数据存储、查询及数据标准化等功能的建设，本人作为项目经办负责技术方案的管理和设计。征信管理平台是我行风险控制的重要部分，维护有个人、对公征信、互联网小贷、电信运营商等数据。征信管理平台承载了大量业务在风险控制环节对数据的诉求，如新发卡、贷前、贷中等关键业务流程。本文介绍了几种主要架构风格及特点，论述了该项目在软件架构选择过程中，为何选择了三种风格的组合。最后，文章总结了采用该组合风格后带来的好处及缺点。","text":"论文摘要 2017年4月份，某股份制银行启动了新征信管理平台的系统项目，本人所在的大数据支持处室负责了数据存储、查询及数据标准化等功能的建设，本人作为项目经办负责技术方案的管理和设计。征信管理平台是我行风险控制的重要部分，维护有个人、对公征信、互联网小贷、电信运营商等数据。征信管理平台承载了大量业务在风险控制环节对数据的诉求，如新发卡、贷前、贷中等关键业务流程。本文介绍了几种主要架构风格及特点，论述了该项目在软件架构选择过程中，为何选择了三种风格的组合。最后，文章总结了采用该组合风格后带来的好处及缺点。 论文正文 背景介绍 随着银行业的快速发展，各业务部对风险管理的要求越来越高，需要更加丰富的征信数据帮忙完成业务流程，同时旧征信平台中使用的传统应用和数据库架构已无法满足业务发展的需求，因此行内提出了建设新征信管理平台的诉求。大量的征信外部数据也是对大数据平台数据源的一个完善，因此我所在的处室承担了相关数据存储、查询及数据标准化等功能的建设（本文中仅对实时应用部分讨论，离线部分不具体展开），本人负责技术方案的管理和设计。 主要架构风格介绍 在基本需求确认后，我们开始了系统架构的选择，首先是对常用的架构风格进行了分析。管道/过滤器风格，每个构件都有一组输入和输出，构件读输入的数据流，经过内部处理，然后产生输出数据流。这里的构件被称为过滤器，这种风格的连接件就像是数据流传输的管道，将一个过滤器的输出传到另一过滤器的输入。基于事件的隐式调用风格，其思想是构件不直接调用一个过程，而是触发或广播一个或多个事件。系统中的其它构件中的过程在一个或多个事件中注册，当一个事件被触发，系统自动调用在这个事件中注册的所有过程，这样，一个事件的触发就导致了另一模块中的过程的调用。微服务风格，是指开发一个单个小型的但有业务功能的服务，每个服务都有自己的处理和轻量通讯机制，可以部署在单个或多个服务器上。微服务也指一种种松耦合的、有一定的有界上下文的面向服务架构。三层分层架构，将整个业务应用划分为界面层、业务逻辑层、数据访问层，目的是为了“高内聚低耦合”的思想。 架构风格选择 结合业务需求和行内现有技术框架及系统环境的实际情况，最终我们再整体上选择了微服务架构，部分模块中使用分层架构，事件驱动，数据库系统这三种风格的混合模式。 在系统整体架构选择上，主要考虑了系统的可用性、性能及可扩展性，采用了微服务的风格：1、查询服务按业务领域进行划分，有征信服务、运营商服务、个人属性服务等负责具体数据的查询、主备供应商的切换及数据的标准化，并且将查询后的数据及相关的日志信息防止队列中间件中；2、数据回写服务负责从队列将数据入库，并根据规则生成有效期等字段，以便数据复用。3、服务治理的相关组件，注册中心负责管理服务的元信息；配置中心负责管理各服务的配置文件；网关负责交易请求的路由。4、业务数据存储上采用了分布式的中间件Elasticsearch，过程中直接存取非结构化数据，减少关联查询。按业务领域划分查询服务可以使得扩展更加灵活，不同业务领域的修改不会互相影响，当后续不同领域的数据供应商新增时，只需要新增服务，不需要修改远服务减少影响范围。同时个别繁忙服务需要扩充资源时，也能灵活扩充，不会造成浪费。 在面向业务使用的后台管理服务中，首先我们为了安装方便，采用了B/S架构，整体上将系统分为页面表现层，业务逻辑层及数据持久层这三层。表现层采用jsp文件实现，业务逻辑层为一组java service，数据持久层采用mybatis xml文件实现。这样设计的原因是可以降低层与层之间的依赖，便于各层逻辑的复用，在后期维护的时候，极大地降低了维护成本和维护时间 实际业务需求调研时我们了解到用户在多个地方有审批需求，比如业务机会阶段变更到2阶段时，项目名称修改时，及项目非正常关闭时，考虑到可能还会有其他地方需要审批，而审批是一个比较独立的功能模块，有自己的一系列行为，审批功能可能也会随着外部比如OA系统升级而做修改或替换。所以这里我们采用了事件驱动架构，将审批功能分装成一个独立构件，将启动审批流程过程注册到审批事件中，各个需要的功能界面只需触发审批事件即可。 总结 项目耗时4个月时间最终成功上线，上线后得到使用单位用户和领导的高度认可，至今使用已经超过两年，后期维护过程中，用户单位由于业务分工及组织架构调整，项目池要求由原先的按照行业来划分修改为按照地区来划分，由于我们将项目池分类定义在数据库系统中，所以我们很快地就满足了用户的修改需求。另外，我们发现到采用该混合架构也存在一些缺陷，比如B/S分层架构带来的页面查询性能不佳，随着数据量的增加，首页刷新时间一度增加到8秒以上，针对这个问题，我们后来将首页分成几个块用多线程的方式分别独立刷新，现在速度提高至3秒左右。 通过这个项目，本人更进一步了解到系统架构设计的重要性，也意识到各类架构风格只为满足核心业务场景系统需求的本质。","categories":[],"tags":[]},{"title":"《Kafka技术内幕》--读书笔记5","slug":"《Kafka技术内幕》-读书笔记5","date":"2018-09-12T11:27:39.000Z","updated":"2020-06-06T13:24:36.346Z","comments":true,"path":"2018/09/12/《Kafka技术内幕》-读书笔记5/","link":"","permalink":"https://awdclijn.github.io/2018/09/12/《Kafka技术内幕》-读书笔记5/","excerpt":"消费者的配置信息要指定连接的ZK集群以及消费组编号。消费者客户端会通过消费者连接器(ConsumerConnector)连接ZK集群，获取分配的分区，创建每个主题对应的消息流(KafkaStream),最后迭代消息流，读取每条消息，并完成具体的业务处理逻辑(这里只是简单地打印出收到的每条信息)。 消费者客户端通过消费者连接器读取消息的具体步骤如下。 消费者的配置信息指定订阅的主题和主题对应的线程数，每个线程对应一个消息流。 Consumer对象通过配置文件创建基于ZK的消费者连接器。 消费者连接器根据主题和线程数创建多个消息流。 在每个消息流通过循环消费者迭代器(ConsumerIterator)读出消息。","text":"消费者的配置信息要指定连接的ZK集群以及消费组编号。消费者客户端会通过消费者连接器(ConsumerConnector)连接ZK集群，获取分配的分区，创建每个主题对应的消息流(KafkaStream),最后迭代消息流，读取每条消息，并完成具体的业务处理逻辑(这里只是简单地打印出收到的每条信息)。 消费者客户端通过消费者连接器读取消息的具体步骤如下。 消费者的配置信息指定订阅的主题和主题对应的线程数，每个线程对应一个消息流。 Consumer对象通过配置文件创建基于ZK的消费者连接器。 消费者连接器根据主题和线程数创建多个消息流。 在每个消息流通过循环消费者迭代器(ConsumerIterator)读出消息。 创建并初始化消费者连接器默认的消费者连接器实现类是ZookeeperConsumerConnector，消费者连接器还会协调下面的各个组件来读取消息。 listeners。注册主题分区的更新、会话超时、消费者成员变化事件，触发再平衡。 zkUtils。从ZK中获取主题、分区、消费者列表，为再平衡时的分区分配提供决策。 topicRegistry。消费者分配的分区，结构是“主题→(分区→分区信息)”。 fetcher。消费者拉取线程的管理类，拉取线程会向服务端拉取分区的消息。 topicThreadldAndQueues。消费者订阅的主题和线程数，每个线程对应一个队列。 offsetsChannel。偏移量存储为Kafka内部主题时，需要和管理消费组的协调者通信。 监听器(1)是消息消费事件的导火索，一旦触发了再平衡，需要从ZK中读取所有的分区和已注册的消费者(2)。然后通过分区分配算法，每个消费者都会分配到不同的分区列表(3)。接着拉取线程开始拉取对应的分区消息(4)，并将拉取到的消息放到每个线程的队列中(5)，最后消费者客户端就可以从队列中读取出消息了。另外，为了及时保存消费进度，我们还需要将偏移量保存至offsetsChannel通道对应的节点中(6)。 消费者客户端的线程模型消费者连接器的createMessageStreams()方法会调用consume()方法，但consume()方法并不真正消费数据，而只是为消费消息做准备工作。 根据客户端传入的topicCountMap构造对应的队列和消息流，消息流引用了队列。 在ZK的消费组父节点下注册消费者子节点。 执行初始化工作，触发再平衡，为消费者分配分区，拉取线程会拉取消息放到队列中。 返回消息流列表，队列中有数据时，客户端就可以从消息流中迭代读取消息。 消费者客户端线程模型的主要概念有消费者线程、队列、消息流，这三者的关系都是一一对应的。如果将线程模型和服务端的分区再结合起来，一个线程允许分配多个分区，那么多个分区会共用同一个线程对应的一个队列和一个消息流。下面我们分析几个和消费者线程模型相关的变量。 topicCountMap，设置主题及其对应的线程个数，每个钱程都对应一个队列和一个消息流。 consumer时，即“消费者编号”，用“消费组名称+随机值”表示，指定消费者在消费组中的唯一编号。 ConsumerThreadid，即“消费者线程编号”，用“消费者编号+线程编号”表示。 consumerThreadidsPerTopicMap，表示每个主题和消费者线程编号集合的映射关系。 topicThreadids，表示所有的消费者线程编号集合，相同主题的线程会在同一个数组里。 topicThreadidAndQueues，表示消费者线程和队列的映射关系，因为每个线程对应一个队列。 消费者客户端只需要指主订阅的主题和线程数量，具体主题分成几个分区、线程分配到了哪些分区、分区分布在哪些节点上，对客户端都是透明的。客户端的关注点是每个线程都对应一个队列，每个队列都对应了一个消息流，只要队列中有数据，就能从消息流中迭代读取出消息。 队列作为消息流和拉取线程的共享内存数据结构，会通过消费者连接器的topicThreadldAndQueues全局引用，传递到拉取线程。当拉取线程往队列中填充数据时，消费者客户端就可以通过消息流从队列读取消息。 重新初始化消费者消费者连接器的consume()方法在注册消费者至ZK后，调用reinitializeConsumer()方法执行重新初始化。消费者启动时希望被加入消费组，必须执行一次初始化方法，并触发消费组内所有消费者成员(当然也包括自己)的再平衡。 每个消费者在启动时都要订阅3种事件:会话超时事件、消费组的子节点变化事件(消费者增减)、主题的数据变化事件(分区增减)。这3种事件任何一个发生，都会触发再平衡操作。如果从消费组级别来看，其他消费者也会订阅这些事件，也都会发生再平衡。即消费组中的所有消费者都会发生再平衡。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"《Kafka技术内幕》--读书笔记4","slug":"《Kafka技术内幕》-读书笔记4","date":"2018-09-08T08:21:23.000Z","updated":"2020-06-06T13:24:35.134Z","comments":true,"path":"2018/09/08/《Kafka技术内幕》-读书笔记4/","link":"","permalink":"https://awdclijn.github.io/2018/09/08/《Kafka技术内幕》-读书笔记4/","excerpt":"Kafka集群的数据需要被不同类型的消费者使用，而不同类型的消费者处理逻辑不同。Kafka使用消费组的概念，允许一组消费者进程对消费工作进行划分。每个消费者都可以配置一个所属的消费组，并且订阅多个主题。Kafka会发送每条消息给每个消费组中的一个消费者迫二程(同一条消息广播给多个消费组，单播给同一组中的消费者)。被订阅主题的所有分区会平均地负载给订阅方，即消费组中的所有消费者。 Kafka采用消费组保证了“一个分区只可被消费组中的一个消费者所消费”，这意味着: 在一个消费组中，一个消费者可以消费多个分区。 不同的消费者消费的分区一定不会重复，所有消费者一起消费所有的分区。 在不同消费组中，每个消费组都会悄费所有的分区。 同一个消费组下消费者对分区是互斥的，而不同消费组之间是共享的。","text":"Kafka集群的数据需要被不同类型的消费者使用，而不同类型的消费者处理逻辑不同。Kafka使用消费组的概念，允许一组消费者进程对消费工作进行划分。每个消费者都可以配置一个所属的消费组，并且订阅多个主题。Kafka会发送每条消息给每个消费组中的一个消费者迫二程(同一条消息广播给多个消费组，单播给同一组中的消费者)。被订阅主题的所有分区会平均地负载给订阅方，即消费组中的所有消费者。 Kafka采用消费组保证了“一个分区只可被消费组中的一个消费者所消费”，这意味着: 在一个消费组中，一个消费者可以消费多个分区。 不同的消费者消费的分区一定不会重复，所有消费者一起消费所有的分区。 在不同消费组中，每个消费组都会悄费所有的分区。 同一个消费组下消费者对分区是互斥的，而不同消费组之间是共享的。 Kafka实现传统队列的方式: 发布-订阅模式。同一条消息会被多个消费组消费，每个消费组只有一个消费者，实现广播。 队列模式。只有一个消费组、多个消费者一条消息只被消费组的一个消费者消费，实现单播。 一旦出现消费组内消费者的调整，消费组内的消费者需要执行再平衡的工作。再平衡操作针对的是消费组中的所有消费者，所有消费者都妥执行重新分配分区的动作。再平衡前的消费者保存了分区的消费进度，再平衡后的消费者就可以从保存的进度位置继续读取分区。 生产者的提交日志采用递增的偏移量，连同消息内容一起写入本地日志文件。生产者客户端不需要保存偏移量相关的状态，消费者客户端则要保存消费消息的偏移盘即消费进度。消费进度表示消费者对一个分区已经消费到了哪里。 消费者对分区的消费进度通常保存在外部存储系统中，比如ZK或者Kafka的内部主题(consume_offsets)。 一个分区只能属于一个消费者线程，将分区分配给消费者有以下几种场景。 线程数量多于分区的数量，有部分钱程无法消费该主题下任何一条消息。 线程数量少于分区的数量，有一些线程会消费多个分区的数据。 线程数量等于分区的数量，则正好一个钱程消费一个分区的数据。 一个消费者线程消费多个分区，可以保证消费同一个分区的消息一定是有序的，但并不保证消费者接收到多个分区的消息完全有序。 消费者除了需要保存消费进度到ZK中，它分配的分区也是从ZK读取的。ZK不仅存储了Kafka的内部元数据，而且记录了消费组的成员列表、分区的消费进度、分区的所有者。表3-1总结了消息代理节点、主题、分区、消费者、偏移量(offset)、所有权(ownership)在ZK中的注册信息。 高级API。消费者客户端代码不需要管理偏移量的提交，并且采用了消费组的向动负载均衡功能，确保消费者的增减不会影响消息的消费。高级API提供了从Kafka消费数据的高层抽象。 低级API。通常针对特殊的消费逻辑，比如消费者只想消费某些特定的分区。低级API的客户端代码需要自己实现一些和Kafka服务端相关的底层逻辑，比如选择分区的主剧本、处理主副本的故障转移等也","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"《Kafka技术内幕》--读书笔记3","slug":"《Kafka技术内幕》-读书笔记3","date":"2018-08-13T14:54:43.000Z","updated":"2018-08-27T15:53:50.000Z","comments":true,"path":"2018/08/13/《Kafka技术内幕》-读书笔记3/","link":"","permalink":"https://awdclijn.github.io/2018/08/13/《Kafka技术内幕》-读书笔记3/","excerpt":"客户端消息发送线程我们先按照分区的主副本节点进行分组，把属于同一个节点的所有分区放在一起，合并成一个请求发送。","text":"客户端消息发送线程我们先按照分区的主副本节点进行分组，把属于同一个节点的所有分区放在一起，合并成一个请求发送。 消息被记录收集器收集，并按照分区追加到队列的最后一个批记录中。 发送钱程通过ready()从记录收集器中找出已经准备好的服务端节点。 节点已经准备好，如果客户端还没有和它们建立连接，通过connect()建立到服务端的连接。 发送线程通过drain()从记录收集器获取按照节点整理好的每个分区的批记录。 发送线程得到每个节点的批记录后，为每个节点创建客户端请求，并将请求发送到服务端。 创建生产者客户端请求发送线程并不负责真正发送客户端请求，它会从记录收集器中取出要发送的消息，创建好客户端请求，然后把请求交给客户端网络对象(NetworkClient)去发送。因为没有在发送线程中发送请求，所以创建客户端请求时需要保留目标节点，这样客户端网络对象获取出客户端请求时，才能知道要发送给哪个目标节点。 准备发送客户端请求客户端向服务端发送请求需要先建立网络连接。如果服务端还没有准备好，即还不能连接，这个节点在客户端就会被移除掉，确保消息不会发送给还没有准备好的节点;如果服务端已经准备好了，则调用selector.connect()方法建立到目标节点的网络连接。 /** * Begin connecting to the given node, return true if we are already connected and ready to send to that node. * * @param node The node to check * @param now The current timestamp * @return True if we are ready to send to the given node */ @Override public boolean ready(Node node, long now) &#123; if (node.isEmpty()) throw new IllegalArgumentException(\"Cannot connect to empty node \" + node); if (isReady(node, now)) return true; if (connectionStates.canConnect(node.idString(), now)) // if we are interested in sending to a node and we don't have a connection to it, initiate one initiateConnect(node, now); return false; &#125; 这一步只是将请求暂存到节点对应的网络通道中，还没有真正地将客户端请求发送出去。 private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long now) &#123; String nodeId = clientRequest.destination(); if (!isInternalRequest) &#123; // If this request came from outside the NetworkClient, validate // that we can send data. If the request is internal, we trust // that that internal code has done this validation. Validation // will be slightly different for some internal requests (for // example, ApiVersionsRequests can be sent prior to being in // READY state.) if (!canSendRequest(nodeId)) throw new IllegalStateException(\"Attempt to send a request to node \" + nodeId + \" which is not ready.\"); &#125; AbstractRequest request = null; AbstractRequest.Builder&lt;?&gt; builder = clientRequest.requestBuilder(); try &#123; NodeApiVersions versionInfo = nodeApiVersions.get(nodeId); // Note: if versionInfo is null, we have no server version information. This would be // the case when sending the initial ApiVersionRequest which fetches the version // information itself. It is also the case when discoverBrokerVersions is set to false. if (versionInfo == null) &#123; if (discoverBrokerVersions &amp;&amp; log.isTraceEnabled()) log.trace(\"No version information found when sending message of type &#123;&#125; to node &#123;&#125;. \" + \"Assuming version &#123;&#125;.\", clientRequest.apiKey(), nodeId, builder.version()); &#125; else &#123; short version = versionInfo.usableVersion(clientRequest.apiKey()); builder.setVersion(version); &#125; // The call to build may also throw UnsupportedVersionException, if there are essential // fields that cannot be represented in the chosen version. request = builder.build(); &#125; catch (UnsupportedVersionException e) &#123; // If the version is not supported, skip sending the request over the wire. // Instead, simply add it to the local queue of aborted requests. log.debug(\"Version mismatch when attempting to send &#123;&#125; to &#123;&#125;\", clientRequest.toString(), clientRequest.destination(), e); ClientResponse clientResponse = new ClientResponse(clientRequest.makeHeader(), clientRequest.callback(), clientRequest.destination(), now, now, false, e, null); abortedSends.add(clientResponse); return; &#125; RequestHeader header = clientRequest.makeHeader(); if (log.isDebugEnabled()) &#123; int latestClientVersion = ProtoUtils.latestVersion(clientRequest.apiKey().id); if (header.apiVersion() == latestClientVersion) &#123; log.trace(\"Sending &#123;&#125; to node &#123;&#125;.\", request, nodeId); &#125; else &#123; log.debug(\"Using older server API v&#123;&#125; to send &#123;&#125; to node &#123;&#125;.\", header.apiVersion(), request, nodeId); &#125; &#125; Send send = request.toSend(nodeId, header); InFlightRequest inFlightRequest = new InFlightRequest( header, clientRequest.createdTimeMs(), clientRequest.destination(), clientRequest.callback(), clientRequest.expectResponse(), isInternalRequest, send, now); this.inFlightRequests.add(inFlightRequest); selector.send(inFlightRequest.send); &#125; 针对同一个服务端，如果上一个客户端请求还没有发送完成，则不允许发送新的客户端请求。客户端网络连接对象用inFlightRequsts变量在客户端缓存了还没有收到响应的客户端请求，InFlightRequests类包含一个节点到双端队列的映射结构。在准备发送客户端请求时，请求将添加到指定节点对应的队列中;在收到响应后，才会将请求从队列中移除。 客户端轮询并调用回调函数发送线程run()方法的最后一步是调用NetworkClient的poll()方法。轮询的最关键步骤是调用selector.poll()方法，而在轮询之后，定义了多个处理方法。轮询不仅仅会发送客户端请求，也会接收客户端响应。客户端发送请求后会调用handleCompletedSends()处理已经完成的发送，客户端接收到响应后会调用handleCompletedReceives()处理已经完成的接收。如果客户端发送完请求不需要响应，在处理已经完成的发送时，就会将对应的请求从iFlightRequests队列中移踪。而因为没有响应结果，也就不会有机会调用handleCompletedReceives()方法。如果客户端请求需要响应，则只有在handleCompletedReceives()中才会删除对应的请求:因为inFlightRequests队列保存的是未收到响应的客户端请求，请求已经有响应，就不需要存在于队列中。 /** * Do actual reads and writes to sockets. * * @param timeout The maximum amount of time to wait (in ms) for responses if there are none immediately, * must be non-negative. The actual timeout will be the minimum of timeout, request timeout and * metadata timeout * @param now The current time in milliseconds * @return The list of responses received */ @Override public List&lt;ClientResponse&gt; poll(long timeout, long now) &#123; long metadataTimeout = metadataUpdater.maybeUpdate(now); try &#123; this.selector.poll(Utils.min(timeout, metadataTimeout, requestTimeoutMs)); &#125; catch (IOException e) &#123; log.error(\"Unexpected error during I/O\", e); &#125; // process completed actions long updatedNow = this.time.milliseconds(); List&lt;ClientResponse&gt; responses = new ArrayList&lt;&gt;(); handleAbortedSends(responses); handleCompletedSends(responses, updatedNow); handleCompletedReceives(responses, updatedNow); handleDisconnections(responses, updatedNow); handleConnections(); handleInitiateApiVersionRequests(updatedNow); handleTimedOutRequests(responses, updatedNow); // invoke callbacks // 上面几个处理都会往responses中添加数据，有了响应后开始调用请求的回调函数 for (ClientResponse response : responses) &#123; try &#123; response.onComplete(); &#125; catch (Exception e) &#123; log.error(\"Uncaught error in request completion:\", e); &#125; &#125; return responses; &#125; 不需要响应的流程。开始发送请求→添加客户端请求到队列→发送请求→请求发送成功→从队列中删除发送请求→构造客户端响应。 需要晌应的流程。开始发送请求→添加客户端请求到队列→发送请求→请求发送成功→等待接收响应→接收响应→接收到完整的响应→从队列中删除客户端请求→构造客户端响应。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"《Kafka技术内幕》--读书笔记2","slug":"《Kafka技术内幕》-读书笔记2","date":"2018-07-18T14:42:29.000Z","updated":"2018-08-06T14:54:50.000Z","comments":true,"path":"2018/07/18/《Kafka技术内幕》-读书笔记2/","link":"","permalink":"https://awdclijn.github.io/2018/07/18/《Kafka技术内幕》-读书笔记2/","excerpt":"Kafka初期使用Scala编写，早期Scala版本的生产者、消费者和服务端的实现都放在core包下;而最新的客户端使用了Java重新实现，放在clients包下。 新生产者新的生产者应用程序使用KafkaProducer对象代表一个生产者客户端进程。生产者要发送消息，并不是直接发送给服务端，而是先在客户端把消息放入队列中，然后由一个消息发送线程从队列中拉取消息，以批量的方式发送消息给服务端。Kafka的记录收集器(RecordAccumulator)负责缓存生产者客户端产生的消息，发送线程(Sender)负责读取记录收集器的批量消息，通过网络发送给服务端。为了保证客户端网络请求的快速响应，Kafka使用选择器(Selector)处理网络连接和读写处理，使用网络连接(NetworkClient)处理客户端网络请求。","text":"Kafka初期使用Scala编写，早期Scala版本的生产者、消费者和服务端的实现都放在core包下;而最新的客户端使用了Java重新实现，放在clients包下。 新生产者新的生产者应用程序使用KafkaProducer对象代表一个生产者客户端进程。生产者要发送消息，并不是直接发送给服务端，而是先在客户端把消息放入队列中，然后由一个消息发送线程从队列中拉取消息，以批量的方式发送消息给服务端。Kafka的记录收集器(RecordAccumulator)负责缓存生产者客户端产生的消息，发送线程(Sender)负责读取记录收集器的批量消息，通过网络发送给服务端。为了保证客户端网络请求的快速响应，Kafka使用选择器(Selector)处理网络连接和读写处理，使用网络连接(NetworkClient)处理客户端网络请求。 发送消息Kafka源码根目录的examples包public class Producer extends Thread &#123; private final KafkaProducer&lt;Integer, String&gt; producer; private final String topic; private final Boolean isAsync; public Producer(String topic, Boolean isAsync) &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", KafkaProperties.KAFKA_SERVER_URL + \":\" + KafkaProperties.KAFKA_SERVER_PORT); props.put(\"client.id\", \"DemoProducer\"); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.IntegerSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); producer = new KafkaProducer&lt;&gt;(props); this.topic = topic; this.isAsync = isAsync; &#125; public void run() &#123; int messageNo = 1; while (true) &#123; String messageStr = \"Message_\" + messageNo; long startTime = System.currentTimeMillis(); if (isAsync) &#123; // Send asynchronously producer.send(new ProducerRecord&lt;&gt;(topic, messageNo, messageStr), new DemoCallBack(startTime, messageNo, messageStr)); &#125; else &#123; // Send synchronously try &#123; producer.send(new ProducerRecord&lt;&gt;(topic, messageNo, messageStr)).get(); System.out.println(\"Sent message: (\" + messageNo + \", \" + messageStr + \")\"); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; ++messageNo; &#125; &#125; &#125; class DemoCallBack implements Callback &#123; private final long startTime; private final int key; private final String message; public DemoCallBack(long startTime, int key, String message) &#123; this.startTime = startTime; this.key = key; this.message = message; &#125; /** * A callback method the user can implement to provide asynchronous handling of request completion. This method will * be called when the record sent to the server has been acknowledged. Exactly one of the arguments will be * non-null. * * @param metadata The metadata for the record that was sent (i.e. the partition and offset). Null if an error * occurred. * @param exception The exception thrown during processing of this record. Null if no error occurred. */ public void onCompletion(RecordMetadata metadata, Exception exception) &#123; long elapsedTime = System.currentTimeMillis() - startTime; if (metadata != null) &#123; System.out.println( \"message(\" + key + \", \" + message + \") sent to partition(\" + metadata.partition() + \"), \" + \"offset(\" + metadata.offset() + \") in \" + elapsedTime + \" ms\"); &#125; else &#123; exception.printStackTrace(); &#125; &#125; &#125; KafkaProducer用send方法，完成同步和l异步两种模式的消息发迭。因为send方法返回的是一个Future。基于Future，我们可以实现同步或异步的消息发送语义。 同步。调用send返回Future时，需要立即调用get，因为Future.get在没有返回结果时会一直阻塞。 异步。提供一个回调，调用send后可以继续发送消息而不用等待。当有结果运回时，会向动执行回调函数。 发送消息实现：/** * Implementation of asynchronously send a record to a topic. */ private Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; TopicPartition tp = null; try &#123; // first make sure the metadata for the topic is available // 更新对应topic的元数据 ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs); long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs); Cluster cluster = clusterAndWaitTime.cluster; byte[] serializedKey; try &#123; serializedKey = keySerializer.serialize(record.topic(), record.key()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(\"Can't convert key of class \" + record.key().getClass().getName() + \" to class \" + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + \" specified in key.serializer\"); &#125; byte[] serializedValue; try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.value()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(\"Can't convert value of class \" + record.value().getClass().getName() + \" to class \" + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + \" specified in value.serializer\"); &#125; int partition = partition(record, serializedKey, serializedValue, cluster); int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue); ensureValidRecordSize(serializedSize); tp = new TopicPartition(record.topic(), partition); long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp(); log.trace(\"Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;\", record, callback, record.topic(), partition); // producer callback will make sure to call both 'callback' and interceptor callback Callback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp); RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs); if (result.batchIsFull || result.newBatchCreated) &#123; log.trace(\"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch\", record.topic(), partition); this.sender.wakeup(); &#125; return result.future; // handling exceptions and record the errors; // for API exceptions return them in the future, // for other exceptions throw directly &#125; catch (ApiException e) &#123; log.debug(\"Exception occurred during message send:\", e); if (callback != null) callback.onCompletion(null, e); this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); return new FutureFailure(e); &#125; catch (InterruptedException e) &#123; this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw new InterruptException(e); &#125; catch (BufferExhaustedException e) &#123; this.errors.record(); this.metrics.sensor(\"buffer-exhausted-records\").record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (KafkaException e) &#123; this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (Exception e) &#123; // we notify interceptor about all exceptions, since onSend is called before anything else in this method if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125; &#125; 序列化，按配置加载序列化的类if (keySerializer == null) &#123; this.keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, Serializer.class); this.keySerializer.configure(config.originals(), true); &#125; else &#123; config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG); this.keySerializer = keySerializer; &#125; if (valueSerializer == null) &#123; this.valueSerializer = config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, Serializer.class); this.valueSerializer.configure(config.originals(), false); &#125; else &#123; config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG); this.valueSerializer = valueSerializer; &#125; 计算消息要落到哪个partition/** * Compute the partition for the given record. * * @param topic The topic name * @param key The key to partition on (or null if no key) * @param keyBytes serialized key to partition on (or null if no key) * @param value The value to partition on or null * @param valueBytes serialized value to partition on or null * @param cluster The current cluster metadata */ public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125; &#125; 计算消息长度是否合法int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue); ensureValidRecordSize(serializedSize); 等待批量发送消息tp = new TopicPartition(record.topic(), partition); long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp(); log.trace(\"Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;\", record, callback, record.topic(), partition); // producer callback will make sure to call both 'callback' and interceptor callback Callback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp); RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs); if (result.batchIsFull || result.newBatchCreated) &#123; log.trace(\"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch\", record.topic(), partition); this.sender.wakeup(); &#125; return result.future; 生产者发迭的消息先在客户端缓存到记录收集器RecordAccumulator中，等到一定时机再由发送线程Sender批量地写入Kafka集群。生产者每生产一条消息，就向记录收集器中追加一条消息，追加方法 的返回值表示批记录(RecordBatch)是否满了:如果批记录满了，则开始发送这一批数据。每个分区都有一个双端队列用来缓存客户端的消息，队列的每个元素是一个批记录。一旦分区的队列中有批记录满了，就会被发送线程发送到分区对应的节点;如果批记录没有满，就会继续等待直到收集到足够的消息。 追加消息时首先要获取分区所属的队列，然后取队列中最后一个批记录，如果队列中不存在批记录或者上一个批记录已经写满，应该创建新的批记录，并且加入队列的尾部。 // We keep track of the number of appending thread to make sure we do not miss batches in // abortIncompleteBatches(). appendsInProgress.incrementAndGet(); try &#123; // check if we have an in-progress batch Deque&lt;RecordBatch&gt; dq = getOrCreateDeque(tp); synchronized (dq) &#123; if (closed) throw new IllegalStateException(\"Cannot send after the producer is closed.\"); RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq); if (appendResult != null) return appendResult; &#125; // we don't have an in-progress record batch try to allocate a new batch int size = Math.max(this. , Records.LOG_OVERHEAD + Record.recordSize(key, value)); log.trace(\"Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;\", size, tp.topic(), tp.partition()); ByteBuffer buffer = free.allocate(size, maxTimeToBlock); synchronized (dq) &#123; // Need to check if producer is closed again after grabbing the dequeue lock. if (closed) throw new IllegalStateException(\"Cannot send after the producer is closed.\"); RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq); if (appendResult != null) &#123; // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often... free.deallocate(buffer); return appendResult; &#125; MemoryRecordsBuilder recordsBuilder = MemoryRecords.builder(buffer, compression, TimestampType.CREATE_TIME, this.batchSize); RecordBatch batch = new RecordBatch(tp, recordsBuilder, time.milliseconds()); FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, callback, time.milliseconds())); dq.addLast(batch); incomplete.add(batch); return new RecordAppendResult(future, dq.size() &gt; 1 || batch.isFull(), true); &#125; &#125; finally &#123; appendsInProgress.decrementAndGet(); &#125; /** * If `RecordBatch.tryAppend` fails (i.e. the record batch is full), close its memory records to release temporary * resources (like compression streams buffers). */ private RecordAppendResult tryAppend(long timestamp, byte[] key, byte[] value, Callback callback, Deque&lt;RecordBatch&gt; deque) &#123; RecordBatch last = deque.peekLast(); if (last != null) &#123; FutureRecordMetadata future = last.tryAppend(timestamp, key, value, callback, time.milliseconds()); if (future == null) last.close(); else return new RecordAppendResult(future, deque.size() &gt; 1 || last.isFull(), false); &#125; return null; &#125;","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"《Kafka技术内幕》--读书笔记1","slug":"《Kafka技术内幕》-读书笔记1","date":"2018-07-15T02:08:10.000Z","updated":"2020-06-06T13:10:46.698Z","comments":true,"path":"2018/07/15/《Kafka技术内幕》-读书笔记1/","link":"","permalink":"https://awdclijn.github.io/2018/07/15/《Kafka技术内幕》-读书笔记1/","excerpt":"基本概念 生产者 (producer)应用程序发布事件流到Kafka的一个或多个主题。 消费者 (consumer)应用程序订阅Kafka的一个或多个主题，并处理事件流。 连接器 (connector)将Kafka主题和已有数据源进行连接，数据可以互相导人和导出 。 流处理 (processor)从Kafka主题消费输入流，经过处理后，产生输出流到输出主题。","text":"基本概念 生产者 (producer)应用程序发布事件流到Kafka的一个或多个主题。 消费者 (consumer)应用程序订阅Kafka的一个或多个主题，并处理事件流。 连接器 (connector)将Kafka主题和已有数据源进行连接，数据可以互相导人和导出 。 流处理 (processor)从Kafka主题消费输入流，经过处理后，产生输出流到输出主题。 分区模型Kafka集群向多个消息代理服务器(broker server)组成，发布至UKafka集群的每条消息都有一个类别，用主题(topic)来表示。 Kafka集群为每个主题维护了分布式的分区(partition)日志文件，物理意义上可以把主题看作分区的日志文件(partitioned log)。 每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到提交日志(commit log)。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫作偏移盘(offset)，这个偏移量能够唯一地定位当前分区中的每一条消息 。 Kafka以分区作为最小的粒度，将每个分区分配给消费组中不同的而且是唯一的消费者，并确保一个分区只 属于一个消费者，即这个消费者就是这个分区的唯一读取线程 。 消费模型 Kafka采用拉取模型，由消费者向己记录消费状态，每个消费者五相独立地顺序读取每个分区的消息。生产者发布的所有消息会一直保存在Kafka集群中，不管消息有没有被消费。 用户可以通过设置保留时间来清理过期的数据。 分布式模型Kafka每个主题的多个分区日志分布式地存储在Kafka集群上，同时为了故障容错，每个分区都会以副本的方式复制到多个消息代理节点上。其中一个节点会作为主副本(Leader)，其他节点作为备份副本(Follower，也叫作从副本)。主副本会负责所有的客户端读写操作，备份副本仅仅从主副本同步数据。当主副本 出现故障时，备份副本中的一个副本会被选择为新的主副本 。 分区是消费者线程模型的最小并行单位。 其他设计持久化使用“零拷贝技术”(zero-copy)只需将磁盘文件的数据复制到页面缓存中一次，然后将数据从页面缓存直接发送到网络中(发送给不同的使用者时，都可以重复使用同一个页面缓存)，避免了重复的复制操作。 生产者与消费者Kafka的生产者将消息直接发送给分区主副本所在的消息代理节点，并不需要经过任何的中间路由层。为了做到这一点，所有消息代理节点都会保存一份相同的元数据，这份元数据记录了每个主题分区对应的主副本节点。生产者客户端在发送消息之前，会向任意一个代理节点请求元数据，井确定每条消息对应的目标节点然后把消息直接发送给对应的目标节点 。 生产者采用批量发送消息集的方式解决了网络请求过多的问题。生产者会尝试在内存中收集足够数据，并在一个请求中一次性发送一批数据。 Kafka采用了基于拉取模型的消费状态处理，它将主题分成多个有序的分区，任何时刻每个分区都只被一个消费者使用。并且，消费者会记录每个分区的消费进度(即偏移量)。Kafka的消费者会定时地将分区的消费进度保存成检查点文件，表示“这个位置之前的消息都已经被消费过了” 和生产者采用批量发送消息类似，消费者拉取消息也可以一次拉取一批消息。消费者客户端拉取消息，然后处理这一批消息，这个过程一般套在一个死循环里，表示消费者永远处于消费消息的状态 副本和容错备份副本始终尽量保持与主副本的数据同步。备份副本的日志文件和主副本的日志总是相同的，它们都有相同的偏移量和相同顺序的消息。备份副本从主副本消费消息的方式和普通的消费者一样，只不过备份副本会将消息运用到自己的本地日志文件(备份副本和主副本都在服务端，它们都会将收到的分区数据持久化成日志文件)。 kafka对节点的存活定义有两个条件: 节点必须和ZK保持会话; 如果这个节点是某个分区的备份副本，它必须对分区主副本的写操作进行复制，并且复制的进度不能落后太多。 如果一个备份副本挂掉、没有响应或者落后太多，主副本就会将其从同步副本集合中移除。反之，如果备份副本重新赶上主副本，它就会加入到主副本的 同步集合中。 在Kafka中，一条消息只有被ISR集合的所有副本都运用到本地的日志文件，才会认为消息被成功提交了。任何时刻，只要ISR至少有一个副本是存活的， Kafka就可以保证“一条消息一旦被提交，就不会丢失”。只有已经提交的消息才能被消费者消费，因此消费者不用担心会看到因为主副本失败而丢失的消息。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-07-13T12:05:50.012Z","updated":"2018-07-14T06:42:50.836Z","comments":true,"path":"2018/07/13/hello-world/","link":"","permalink":"https://awdclijn.github.io/2018/07/13/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}