{"meta":{"title":"keon随便写写","subtitle":"大概是一些读书笔记","description":null,"author":"keon","url":"https://awdclijn.github.io"},"pages":[{"title":"categories","date":"2018-07-14T05:58:22.000Z","updated":"2018-07-14T06:07:41.138Z","comments":false,"path":"categories/index.html","permalink":"https://awdclijn.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-07-14T05:59:56.000Z","updated":"2018-07-14T06:05:49.671Z","comments":false,"path":"tags/index.html","permalink":"https://awdclijn.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"《Kafka技术内幕》--读书笔记5","slug":"《Kafka技术内幕》-读书笔记5","date":"2018-09-12T11:27:39.000Z","updated":"2018-09-18T15:35:55.691Z","comments":true,"path":"2018/09/12/《Kafka技术内幕》-读书笔记5/","link":"","permalink":"https://awdclijn.github.io/2018/09/12/《Kafka技术内幕》-读书笔记5/","excerpt":"消费者的配置信息要指定连接的ZK集群以及消费组编号。消费者客户端会通过消费者连接器(ConsumerConnector)连接ZK集群，获取分配的分区，创建每个主题对应的消息流(KafkaStream),最后迭代消息流，读取每条消息，并完成具体的业务处理逻辑(这里只是简单地打印出收到的每条信息)。 消费者客户端通过消费者连接器读取消息的具体步骤如下。 消费者的配置信息指定订阅的主题和主题对应的线程数，每个线程对应一个消息流。 Consumer对象通过配置文件创建基于ZK的消费者连接器。 消费者连接器根据主题和线程数创建多个消息流。 在每个消息流通过循环消费者迭代器(ConsumerIterator)读出消息。","text":"消费者的配置信息要指定连接的ZK集群以及消费组编号。消费者客户端会通过消费者连接器(ConsumerConnector)连接ZK集群，获取分配的分区，创建每个主题对应的消息流(KafkaStream),最后迭代消息流，读取每条消息，并完成具体的业务处理逻辑(这里只是简单地打印出收到的每条信息)。 消费者客户端通过消费者连接器读取消息的具体步骤如下。 消费者的配置信息指定订阅的主题和主题对应的线程数，每个线程对应一个消息流。 Consumer对象通过配置文件创建基于ZK的消费者连接器。 消费者连接器根据主题和线程数创建多个消息流。 在每个消息流通过循环消费者迭代器(ConsumerIterator)读出消息。 创建并初始化消费者连接器默认的消费者连接器实现类是ZookeeperConsumerConnector，消费者连接器还会协调下面的各个组件来读取消息。 listeners。注册主题分区的更新、会话超时、消费者成员变化事件，触发再平衡。 zkUtils。从ZK中获取主题、分区、消费者列表，为再平衡时的分区分配提供决策。 topicRegistry。消费者分配的分区，结构是“主题→(分区→分区信息)”。 fetcher。消费者拉取线程的管理类，拉取线程会向服务端拉取分区的消息。 topicThreadldAndQueues。消费者订阅的主题和线程数，每个线程对应一个队列。 offsetsChannel。偏移量存储为Kafka内部主题时，需要和管理消费组的协调者通信。 监听器(1)是消息消费事件的导火索，一旦触发了再平衡，需要从ZK中读取所有的分区和已注册的消费者(2)。然后通过分区分配算法，每个消费者都会分配到不同的分区列表(3)。接着拉取线程开始拉取对应的分区消息(4)，并将拉取到的消息放到每个线程的队列中(5)，最后消费者客户端就可以从队列中读取出消息了。另外，为了及时保存消费进度，我们还需要将偏移量保存至offsetsChannel通道对应的节点中(6)。 消费者客户端的线程模型消费者连接器的createMessageStreams()方法会调用consume()方法，但consume()方法并不真正消费数据，而只是为消费消息做准备工作。 根据客户端传入的topicCountMap构造对应的队列和消息流，消息流引用了队列。 在ZK的消费组父节点下注册消费者子节点。 执行初始化工作，触发再平衡，为消费者分配分区，拉取线程会拉取消息放到队列中。 返回消息流列表，队列中有数据时，客户端就可以从消息流中迭代读取消息。 消费者客户端线程模型的主要概念有消费者线程、队列、消息流，这三者的关系都是一一对应的。如果将线程模型和服务端的分区再结合起来，一个线程允许分配多个分区，那么多个分区会共用同一个线程对应的一个队列和一个消息流。下面我们分析几个和消费者线程模型相关的变量。 topicCountMap，设置主题及其对应的线程个数，每个钱程都对应一个队列和一个消息流。 consumer时，即“消费者编号”，用“消费组名称+随机值”表示，指定消费者在消费组中的唯一编号。 ConsumerThreadid，即“消费者线程编号”，用“消费者编号+线程编号”表示。 consumerThreadidsPerTopicMap，表示每个主题和消费者线程编号集合的映射关系。 topicThreadids，表示所有的消费者线程编号集合，相同主题的线程会在同一个数组里。 topicThreadidAndQueues，表示消费者线程和队列的映射关系，因为每个线程对应一个队列。 消费者客户端只需要指主订阅的主题和线程数量，具体主题分成几个分区、线程分配到了哪些分区、分区分布在哪些节点上，对客户端都是透明的。客户端的关注点是每个线程都对应一个队列，每个队列都对应了一个消息流，只要队列中有数据，就能从消息流中迭代读取出消息。 队列作为消息流和拉取线程的共享内存数据结构，会通过消费者连接器的topicThreadldAndQueues全局引用，传递到拉取线程。当拉取线程往队列中填充数据时，消费者客户端就可以通过消息流从队列读取消息。 重新初始化消费者消费者连接器的consume()方法在注册消费者至ZK后，调用reinitializeConsumer()方法执行重新初始化。消费者启动时希望被加入消费组，必须执行一次初始化方法，并触发消费组内所有消费者成员(当然也包括自己)的再平衡。 每个消费者在启动时都要订阅3种事件:会话超时事件、消费组的子节点变化事件(消费者增减)、主题的数据变化事件(分区增减)。这3种事件任何一个发生，都会触发再平衡操作。如果从消费组级别来看，其他消费者也会订阅这些事件，也都会发生再平衡。即消费组中的所有消费者都会发生再平衡。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"《Kafka技术内幕》--读书笔记4","slug":"《Kafka技术内幕》-读书笔记4","date":"2018-09-08T08:21:23.000Z","updated":"2018-09-11T15:30:33.339Z","comments":true,"path":"2018/09/08/《Kafka技术内幕》-读书笔记4/","link":"","permalink":"https://awdclijn.github.io/2018/09/08/《Kafka技术内幕》-读书笔记4/","excerpt":"Kafka集群的数据需要被不同类型的消费者使用，而不同类型的消费者处理逻辑不同。Kafka使用消费组的概念，允许一组消费者进程对消费工作进行划分。每个消费者都可以配置一个所属的消费组，并且订阅多个主题。Kafka会发送每条消息给每个消费组中的一个消费者迫二程(同一条消息广播给多个消费组，单播给同一组中的消费者)。被订阅主题的所有分区会平均地负载给订阅方，即消费组中的所有消费者。 Kafka采用消费组保证了“一个分区只可被消费组中的一个消费者所消费”，这意味着: 在一个消费组中，一个消费者可以消费多个分区。 不同的消费者消费的分区一定不会重复，所有消费者一起消费所有的分区。 在不同消费组中，每个消费组都会悄费所有的分区。 同一个消费组下消费者对分区是互斥的，而不同消费组之间是共享的。","text":"Kafka集群的数据需要被不同类型的消费者使用，而不同类型的消费者处理逻辑不同。Kafka使用消费组的概念，允许一组消费者进程对消费工作进行划分。每个消费者都可以配置一个所属的消费组，并且订阅多个主题。Kafka会发送每条消息给每个消费组中的一个消费者迫二程(同一条消息广播给多个消费组，单播给同一组中的消费者)。被订阅主题的所有分区会平均地负载给订阅方，即消费组中的所有消费者。 Kafka采用消费组保证了“一个分区只可被消费组中的一个消费者所消费”，这意味着: 在一个消费组中，一个消费者可以消费多个分区。 不同的消费者消费的分区一定不会重复，所有消费者一起消费所有的分区。 在不同消费组中，每个消费组都会悄费所有的分区。 同一个消费组下消费者对分区是互斥的，而不同消费组之间是共享的。 Kafka实现传统队列的方式: 发布-订阅模式。同一条消息会被多个消费组消费，每个消费组只有一个消费者，实现广播。 队列模式。只有一个消费组、多个消费者一条消息只被消费组的一个消费者消费，实现单播。 一旦出现消费组内消费者的调整，消费组内的消费者需要执行再平衡的工作。再平衡操作针对的是消费组中的所有消费者，所有消费者都妥执行重新分配分区的动作。再平衡前的消费者保存了分区的消费进度，再平衡后的消费者就可以从保存的进度位置继续读取分区。 生产者的提交日志采用递增的偏移量，连同消息内容一起写入本地日志文件。生产者客户端不需要保存偏移量相关的状态，消费者客户端则要保存消费消息的偏移盘即消费进度。消费进度表示消费者对一个分区已经消费到了哪里。 消费者对分区的消费进度通常保存在外部存储系统中，比如ZK或者Kafka的内部主题(consume_offsets)。 一个分区只能属于一个消费者线程，将分区分配给消费者有以下几种场景。 线程数量多于分区的数量，有部分钱程无法消费该主题下任何一条消息。 线程数量少于分区的数量，有一些线程会消费多个分区的数据。 线程数量等于分区的数量，则正好一个钱程消费一个分区的数据。 一个消费者线程消费多个分区，可以保证消费同一个分区的消息一定是有序的，但并不保证消费者接收到多个分区的消息完全有序。 消费者除了需要保存消费进度到ZK中，它分配的分区也是从ZK读取的。ZK不仅存储了Kafka的内部元数据，而且记录了消费组的成员列表、分区的消费进度、分区的所有者。表3-1总结了消息代理节点、主题、分区、消费者、偏移量(offset)、所有权(ownership)在ZK中的注册信息。 高级API。消费者客户端代码不需要管理偏移量的提交，并且采用了消费组的向动负载均衡功能，确保消费者的增减不会影响消息的消费。高级API提供了从Kafka消费数据的高层抽象。 低级API。通常针对特殊的消费逻辑，比如消费者只想消费某些特定的分区。低级API的客户端代码需要自己实现一些和Kafka服务端相关的底层逻辑，比如选择分区的主剧本、处理主副本的故障转移等也","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"《Kafka技术内幕》--读书笔记3","slug":"《Kafka技术内幕》-读书笔记3","date":"2018-08-13T14:54:43.000Z","updated":"2018-08-27T15:53:50.000Z","comments":true,"path":"2018/08/13/《Kafka技术内幕》-读书笔记3/","link":"","permalink":"https://awdclijn.github.io/2018/08/13/《Kafka技术内幕》-读书笔记3/","excerpt":"客户端消息发送线程我们先按照分区的主副本节点进行分组，把属于同一个节点的所有分区放在一起，合并成一个请求发送。","text":"客户端消息发送线程我们先按照分区的主副本节点进行分组，把属于同一个节点的所有分区放在一起，合并成一个请求发送。 消息被记录收集器收集，并按照分区追加到队列的最后一个批记录中。 发送钱程通过ready()从记录收集器中找出已经准备好的服务端节点。 节点已经准备好，如果客户端还没有和它们建立连接，通过connect()建立到服务端的连接。 发送线程通过drain()从记录收集器获取按照节点整理好的每个分区的批记录。 发送线程得到每个节点的批记录后，为每个节点创建客户端请求，并将请求发送到服务端。 创建生产者客户端请求发送线程并不负责真正发送客户端请求，它会从记录收集器中取出要发送的消息，创建好客户端请求，然后把请求交给客户端网络对象(NetworkClient)去发送。因为没有在发送线程中发送请求，所以创建客户端请求时需要保留目标节点，这样客户端网络对象获取出客户端请求时，才能知道要发送给哪个目标节点。 准备发送客户端请求客户端向服务端发送请求需要先建立网络连接。如果服务端还没有准备好，即还不能连接，这个节点在客户端就会被移除掉，确保消息不会发送给还没有准备好的节点;如果服务端已经准备好了，则调用selector.connect()方法建立到目标节点的网络连接。 123456789101112131415161718192021/** * Begin connecting to the given node, return true if we are already connected and ready to send to that node. * * @param node The node to check * @param now The current timestamp * @return True if we are ready to send to the given node */@Overridepublic boolean ready(Node node, long now) &#123; if (node.isEmpty()) throw new IllegalArgumentException(\"Cannot connect to empty node \" + node); if (isReady(node, now)) return true; if (connectionStates.canConnect(node.idString(), now)) // if we are interested in sending to a node and we don't have a connection to it, initiate one initiateConnect(node, now); return false;&#125; 这一步只是将请求暂存到节点对应的网络通道中，还没有真正地将客户端请求发送出去。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long now) &#123; String nodeId = clientRequest.destination(); if (!isInternalRequest) &#123; // If this request came from outside the NetworkClient, validate // that we can send data. If the request is internal, we trust // that that internal code has done this validation. Validation // will be slightly different for some internal requests (for // example, ApiVersionsRequests can be sent prior to being in // READY state.) if (!canSendRequest(nodeId)) throw new IllegalStateException(\"Attempt to send a request to node \" + nodeId + \" which is not ready.\"); &#125; AbstractRequest request = null; AbstractRequest.Builder&lt;?&gt; builder = clientRequest.requestBuilder(); try &#123; NodeApiVersions versionInfo = nodeApiVersions.get(nodeId); // Note: if versionInfo is null, we have no server version information. This would be // the case when sending the initial ApiVersionRequest which fetches the version // information itself. It is also the case when discoverBrokerVersions is set to false. if (versionInfo == null) &#123; if (discoverBrokerVersions &amp;&amp; log.isTraceEnabled()) log.trace(\"No version information found when sending message of type &#123;&#125; to node &#123;&#125;. \" + \"Assuming version &#123;&#125;.\", clientRequest.apiKey(), nodeId, builder.version()); &#125; else &#123; short version = versionInfo.usableVersion(clientRequest.apiKey()); builder.setVersion(version); &#125; // The call to build may also throw UnsupportedVersionException, if there are essential // fields that cannot be represented in the chosen version. request = builder.build(); &#125; catch (UnsupportedVersionException e) &#123; // If the version is not supported, skip sending the request over the wire. // Instead, simply add it to the local queue of aborted requests. log.debug(\"Version mismatch when attempting to send &#123;&#125; to &#123;&#125;\", clientRequest.toString(), clientRequest.destination(), e); ClientResponse clientResponse = new ClientResponse(clientRequest.makeHeader(), clientRequest.callback(), clientRequest.destination(), now, now, false, e, null); abortedSends.add(clientResponse); return; &#125; RequestHeader header = clientRequest.makeHeader(); if (log.isDebugEnabled()) &#123; int latestClientVersion = ProtoUtils.latestVersion(clientRequest.apiKey().id); if (header.apiVersion() == latestClientVersion) &#123; log.trace(\"Sending &#123;&#125; to node &#123;&#125;.\", request, nodeId); &#125; else &#123; log.debug(\"Using older server API v&#123;&#125; to send &#123;&#125; to node &#123;&#125;.\", header.apiVersion(), request, nodeId); &#125; &#125; Send send = request.toSend(nodeId, header); InFlightRequest inFlightRequest = new InFlightRequest( header, clientRequest.createdTimeMs(), clientRequest.destination(), clientRequest.callback(), clientRequest.expectResponse(), isInternalRequest, send, now); this.inFlightRequests.add(inFlightRequest); selector.send(inFlightRequest.send);&#125; 针对同一个服务端，如果上一个客户端请求还没有发送完成，则不允许发送新的客户端请求。客户端网络连接对象用inFlightRequsts变量在客户端缓存了还没有收到响应的客户端请求，InFlightRequests类包含一个节点到双端队列的映射结构。在准备发送客户端请求时，请求将添加到指定节点对应的队列中;在收到响应后，才会将请求从队列中移除。 客户端轮询并调用回调函数发送线程run()方法的最后一步是调用NetworkClient的poll()方法。轮询的最关键步骤是调用selector.poll()方法，而在轮询之后，定义了多个处理方法。轮询不仅仅会发送客户端请求，也会接收客户端响应。客户端发送请求后会调用handleCompletedSends()处理已经完成的发送，客户端接收到响应后会调用handleCompletedReceives()处理已经完成的接收。如果客户端发送完请求不需要响应，在处理已经完成的发送时，就会将对应的请求从iFlightRequests队列中移踪。而因为没有响应结果，也就不会有机会调用handleCompletedReceives()方法。如果客户端请求需要响应，则只有在handleCompletedReceives()中才会删除对应的请求:因为inFlightRequests队列保存的是未收到响应的客户端请求，请求已经有响应，就不需要存在于队列中。 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Do actual reads and writes to sockets. * * @param timeout The maximum amount of time to wait (in ms) for responses if there are none immediately, * must be non-negative. The actual timeout will be the minimum of timeout, request timeout and * metadata timeout * @param now The current time in milliseconds * @return The list of responses received */@Overridepublic List&lt;ClientResponse&gt; poll(long timeout, long now) &#123; long metadataTimeout = metadataUpdater.maybeUpdate(now); try &#123; this.selector.poll(Utils.min(timeout, metadataTimeout, requestTimeoutMs)); &#125; catch (IOException e) &#123; log.error(\"Unexpected error during I/O\", e); &#125; // process completed actions long updatedNow = this.time.milliseconds(); List&lt;ClientResponse&gt; responses = new ArrayList&lt;&gt;(); handleAbortedSends(responses); handleCompletedSends(responses, updatedNow); handleCompletedReceives(responses, updatedNow); handleDisconnections(responses, updatedNow); handleConnections(); handleInitiateApiVersionRequests(updatedNow); handleTimedOutRequests(responses, updatedNow); // invoke callbacks // 上面几个处理都会往responses中添加数据，有了响应后开始调用请求的回调函数 for (ClientResponse response : responses) &#123; try &#123; response.onComplete(); &#125; catch (Exception e) &#123; log.error(\"Uncaught error in request completion:\", e); &#125; &#125; return responses;&#125; 不需要响应的流程。开始发送请求→添加客户端请求到队列→发送请求→请求发送成功→从队列中删除发送请求→构造客户端响应。 需要晌应的流程。开始发送请求→添加客户端请求到队列→发送请求→请求发送成功→等待接收响应→接收响应→接收到完整的响应→从队列中删除客户端请求→构造客户端响应。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"《Kafka技术内幕》--读书笔记2","slug":"《Kafka技术内幕》-读书笔记2","date":"2018-07-18T14:42:29.000Z","updated":"2018-08-06T14:54:50.000Z","comments":true,"path":"2018/07/18/《Kafka技术内幕》-读书笔记2/","link":"","permalink":"https://awdclijn.github.io/2018/07/18/《Kafka技术内幕》-读书笔记2/","excerpt":"Kafka初期使用Scala编写，早期Scala版本的生产者、消费者和服务端的实现都放在core包下;而最新的客户端使用了Java重新实现，放在clients包下。 新生产者新的生产者应用程序使用KafkaProducer对象代表一个生产者客户端进程。生产者要发送消息，并不是直接发送给服务端，而是先在客户端把消息放入队列中，然后由一个消息发送线程从队列中拉取消息，以批量的方式发送消息给服务端。Kafka的记录收集器(RecordAccumulator)负责缓存生产者客户端产生的消息，发送线程(Sender)负责读取记录收集器的批量消息，通过网络发送给服务端。为了保证客户端网络请求的快速响应，Kafka使用选择器(Selector)处理网络连接和读写处理，使用网络连接(NetworkClient)处理客户端网络请求。","text":"Kafka初期使用Scala编写，早期Scala版本的生产者、消费者和服务端的实现都放在core包下;而最新的客户端使用了Java重新实现，放在clients包下。 新生产者新的生产者应用程序使用KafkaProducer对象代表一个生产者客户端进程。生产者要发送消息，并不是直接发送给服务端，而是先在客户端把消息放入队列中，然后由一个消息发送线程从队列中拉取消息，以批量的方式发送消息给服务端。Kafka的记录收集器(RecordAccumulator)负责缓存生产者客户端产生的消息，发送线程(Sender)负责读取记录收集器的批量消息，通过网络发送给服务端。为了保证客户端网络请求的快速响应，Kafka使用选择器(Selector)处理网络连接和读写处理，使用网络连接(NetworkClient)处理客户端网络请求。 发送消息Kafka源码根目录的examples包12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class Producer extends Thread &#123; private final KafkaProducer&lt;Integer, String&gt; producer; private final String topic; private final Boolean isAsync; public Producer(String topic, Boolean isAsync) &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", KafkaProperties.KAFKA_SERVER_URL + \":\" + KafkaProperties.KAFKA_SERVER_PORT); props.put(\"client.id\", \"DemoProducer\"); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.IntegerSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); producer = new KafkaProducer&lt;&gt;(props); this.topic = topic; this.isAsync = isAsync; &#125; public void run() &#123; int messageNo = 1; while (true) &#123; String messageStr = \"Message_\" + messageNo; long startTime = System.currentTimeMillis(); if (isAsync) &#123; // Send asynchronously producer.send(new ProducerRecord&lt;&gt;(topic, messageNo, messageStr), new DemoCallBack(startTime, messageNo, messageStr)); &#125; else &#123; // Send synchronously try &#123; producer.send(new ProducerRecord&lt;&gt;(topic, messageNo, messageStr)).get(); System.out.println(\"Sent message: (\" + messageNo + \", \" + messageStr + \")\"); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; ++messageNo; &#125; &#125;&#125;class DemoCallBack implements Callback &#123; private final long startTime; private final int key; private final String message; public DemoCallBack(long startTime, int key, String message) &#123; this.startTime = startTime; this.key = key; this.message = message; &#125; /** * A callback method the user can implement to provide asynchronous handling of request completion. This method will * be called when the record sent to the server has been acknowledged. Exactly one of the arguments will be * non-null. * * @param metadata The metadata for the record that was sent (i.e. the partition and offset). Null if an error * occurred. * @param exception The exception thrown during processing of this record. Null if no error occurred. */ public void onCompletion(RecordMetadata metadata, Exception exception) &#123; long elapsedTime = System.currentTimeMillis() - startTime; if (metadata != null) &#123; System.out.println( \"message(\" + key + \", \" + message + \") sent to partition(\" + metadata.partition() + \"), \" + \"offset(\" + metadata.offset() + \") in \" + elapsedTime + \" ms\"); &#125; else &#123; exception.printStackTrace(); &#125; &#125;&#125; KafkaProducer用send方法，完成同步和l异步两种模式的消息发迭。因为send方法返回的是一个Future。基于Future，我们可以实现同步或异步的消息发送语义。 同步。调用send返回Future时，需要立即调用get，因为Future.get在没有返回结果时会一直阻塞。 异步。提供一个回调，调用send后可以继续发送消息而不用等待。当有结果运回时，会向动执行回调函数。 发送消息实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * Implementation of asynchronously send a record to a topic. */private Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; TopicPartition tp = null; try &#123; // first make sure the metadata for the topic is available // 更新对应topic的元数据 ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs); long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs); Cluster cluster = clusterAndWaitTime.cluster; byte[] serializedKey; try &#123; serializedKey = keySerializer.serialize(record.topic(), record.key()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(\"Can't convert key of class \" + record.key().getClass().getName() + \" to class \" + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + \" specified in key.serializer\"); &#125; byte[] serializedValue; try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.value()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(\"Can't convert value of class \" + record.value().getClass().getName() + \" to class \" + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + \" specified in value.serializer\"); &#125; int partition = partition(record, serializedKey, serializedValue, cluster); int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue); ensureValidRecordSize(serializedSize); tp = new TopicPartition(record.topic(), partition); long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp(); log.trace(\"Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;\", record, callback, record.topic(), partition); // producer callback will make sure to call both 'callback' and interceptor callback Callback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp); RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs); if (result.batchIsFull || result.newBatchCreated) &#123; log.trace(\"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch\", record.topic(), partition); this.sender.wakeup(); &#125; return result.future; // handling exceptions and record the errors; // for API exceptions return them in the future, // for other exceptions throw directly &#125; catch (ApiException e) &#123; log.debug(\"Exception occurred during message send:\", e); if (callback != null) callback.onCompletion(null, e); this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); return new FutureFailure(e); &#125; catch (InterruptedException e) &#123; this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw new InterruptException(e); &#125; catch (BufferExhaustedException e) &#123; this.errors.record(); this.metrics.sensor(\"buffer-exhausted-records\").record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (KafkaException e) &#123; this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (Exception e) &#123; // we notify interceptor about all exceptions, since onSend is called before anything else in this method if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125;&#125; 序列化，按配置加载序列化的类12345678910111213141516if (keySerializer == null) &#123; this.keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, Serializer.class); this.keySerializer.configure(config.originals(), true);&#125; else &#123; config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG); this.keySerializer = keySerializer;&#125;if (valueSerializer == null) &#123; this.valueSerializer = config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, Serializer.class); this.valueSerializer.configure(config.originals(), false);&#125; else &#123; config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG); this.valueSerializer = valueSerializer;&#125; 计算消息要落到哪个partition12345678910111213141516171819202122232425262728/** * Compute the partition for the given record. * * @param topic The topic name * @param key The key to partition on (or null if no key) * @param keyBytes serialized key to partition on (or null if no key) * @param value The value to partition on or null * @param valueBytes serialized value to partition on or null * @param cluster The current cluster metadata */public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125;&#125; 计算消息长度是否合法12int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);ensureValidRecordSize(serializedSize); 等待批量发送消息1234567891011tp = new TopicPartition(record.topic(), partition);long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp();log.trace(\"Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;\", record, callback, record.topic(), partition);// producer callback will make sure to call both 'callback' and interceptor callbackCallback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp);RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs);if (result.batchIsFull || result.newBatchCreated) &#123; log.trace(\"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch\", record.topic(), partition); this.sender.wakeup();&#125;return result.future; 生产者发迭的消息先在客户端缓存到记录收集器RecordAccumulator中，等到一定时机再由发送线程Sender批量地写入Kafka集群。生产者每生产一条消息，就向记录收集器中追加一条消息，追加方法 的返回值表示批记录(RecordBatch)是否满了:如果批记录满了，则开始发送这一批数据。每个分区都有一个双端队列用来缓存客户端的消息，队列的每个元素是一个批记录。一旦分区的队列中有批记录满了，就会被发送线程发送到分区对应的节点;如果批记录没有满，就会继续等待直到收集到足够的消息。 追加消息时首先要获取分区所属的队列，然后取队列中最后一个批记录，如果队列中不存在批记录或者上一个批记录已经写满，应该创建新的批记录，并且加入队列的尾部。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// We keep track of the number of appending thread to make sure we do not miss batches in// abortIncompleteBatches().appendsInProgress.incrementAndGet();try &#123; // check if we have an in-progress batch Deque&lt;RecordBatch&gt; dq = getOrCreateDeque(tp); synchronized (dq) &#123; if (closed) throw new IllegalStateException(\"Cannot send after the producer is closed.\"); RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq); if (appendResult != null) return appendResult; &#125; // we don't have an in-progress record batch try to allocate a new batch int size = Math.max(this. , Records.LOG_OVERHEAD + Record.recordSize(key, value)); log.trace(\"Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;\", size, tp.topic(), tp.partition()); ByteBuffer buffer = free.allocate(size, maxTimeToBlock); synchronized (dq) &#123; // Need to check if producer is closed again after grabbing the dequeue lock. if (closed) throw new IllegalStateException(\"Cannot send after the producer is closed.\"); RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq); if (appendResult != null) &#123; // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often... free.deallocate(buffer); return appendResult; &#125; MemoryRecordsBuilder recordsBuilder = MemoryRecords.builder(buffer, compression, TimestampType.CREATE_TIME, this.batchSize); RecordBatch batch = new RecordBatch(tp, recordsBuilder, time.milliseconds()); FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, callback, time.milliseconds())); dq.addLast(batch); incomplete.add(batch); return new RecordAppendResult(future, dq.size() &gt; 1 || batch.isFull(), true); &#125;&#125; finally &#123; appendsInProgress.decrementAndGet();&#125;/** * If `RecordBatch.tryAppend` fails (i.e. the record batch is full), close its memory records to release temporary * resources (like compression streams buffers). */private RecordAppendResult tryAppend(long timestamp, byte[] key, byte[] value, Callback callback, Deque&lt;RecordBatch&gt; deque) &#123; RecordBatch last = deque.peekLast(); if (last != null) &#123; FutureRecordMetadata future = last.tryAppend(timestamp, key, value, callback, time.milliseconds()); if (future == null) last.close(); else return new RecordAppendResult(future, deque.size() &gt; 1 || last.isFull(), false); &#125; return null;&#125;","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"《Kafka技术内幕》--读书笔记1","slug":"《Kafka技术内幕》-读书笔记1","date":"2018-07-15T02:08:10.000Z","updated":"2018-07-16T13:24:38.736Z","comments":true,"path":"2018/07/15/《Kafka技术内幕》-读书笔记1/","link":"","permalink":"https://awdclijn.github.io/2018/07/15/《Kafka技术内幕》-读书笔记1/","excerpt":"基本概念 生产者 (producer)应用程序发布事件流到Kafka的一个或多个主题。 消费者 (consumer)应用程序订阅Kafka的一个或多个主题，并处理事件流。 连接器 (connector)将Kafka主题和已有数据源进行连接，数据可以互相导人和导出 。 流处理 (processor)从Kafka主题消费输入流，经过处理后，产生输出流到输出主题。","text":"基本概念 生产者 (producer)应用程序发布事件流到Kafka的一个或多个主题。 消费者 (consumer)应用程序订阅Kafka的一个或多个主题，并处理事件流。 连接器 (connector)将Kafka主题和已有数据源进行连接，数据可以互相导人和导出 。 流处理 (processor)从Kafka主题消费输入流，经过处理后，产生输出流到输出主题。 分区模型Kafka集群向多个消息代理服务器(broker server)组成，发布至UKafka集群的每条消息都有一个类别，用主题(topic)来表示。 Kafka集群为每个主题维护了分布式的分区(partition)日志文件，物理意义上可以把主题看作分区的日志文件(partitioned log)。 每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到提交日志(commit log)。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫作偏移盘(offset)，这个偏移量能够唯一地定位当前分区中的每一条消息 。 Kafka以分区作为最小的粒度，将每个分区分配给消费组中不同的而且是唯一的消费者，并确保一个分区只 属于一个消费者，即这个消费者就是这个分区的唯一读取线程 。 消费模型 Kafka采用拉取模型，由消费者向己记录消费状态，每个消费者五相独立地顺序读取每个分区的消息。生产者发布的所有消息会一直保存在Kafka集群中，不管消息有没有被消费。 用户可以通过设置保留时间来清理过期的数据。 分布式模型Kafka每个主题的多个分区日志分布式地存储在Kafka集群上，同时为了故障容错，每个分区都会以副本的方式复制到多个消息代理节点上。其中一个节点会作为主副本(Leader)，其他节点作为备份副本(Follower，也叫作从副本)。主副本会负责所有的客户端读写操作，备份副本仅仅从主副本同步数据。当主副本 出现故障时，备份副本中的一个副本会被选择为新的主副本 。 分区是消费者线程模型的最小并行单位。 其他设计持久化使用“零拷贝技术”(zero-copy)只需将磁盘文件的数据复制到页面缓存中一次，然后将数据从页面缓存直接发送到网络中(发送给不同的使用者时，都可以重复使用同一个页面缓存)，避免了重复的复制操作。 生产者与消费者Kafka的生产者将消息直接发送给分区主副本所在的消息代理节点，并不需要经过任何的中间路由层。为了做到这一点，所有消息代理节点都会保存一份相同的元数据，这份元数据记录了每个主题分区对应的主副本节点。生产者客户端在发送消息之前，会向任意一个代理节点请求元数据，井确定每条消息对应的目标节点然后把消息直接发送给对应的目标节点 。 生产者采用批量发送消息集的方式解决了网络请求过多的问题。生产者会尝试在内存中收集足够数据，并在一个请求中一次性发送一批数据。 Kafka采用了基于拉取模型的消费状态处理，它将主题分成多个有序的分区，任何时刻每个分区都只被一个消费者使用。并且，消费者会记录每个分区的消费进度(即偏移量)。Kafka的消费者会定时地将分区的消费进度保存成检查点文件，表示“这个位置之前的消息都已经被消费过了” 和生产者采用批量发送消息类似，消费者拉取消息也可以一次拉取一批消息。消费者客户端拉取消息，然后处理这一批消息，这个过程一般套在一个死循环里，表示消费者永远处于消费消息的状态 副本和容错备份副本始终尽量保持与主副本的数据同步。备份副本的日志文件和主副本的日志总是相同的，它们都有相同的偏移量和相同顺序的消息。备份副本从主副本消费消息的方式和普通的消费者一样，只不过备份副本会将消息运用到自己的本地日志文件(备份副本和主副本都在服务端，它们都会将收到的分区数据持久化成日志文件)。 kafka对节点的存活定义有两个条件: 节点必须和ZK保持会话; 如果这个节点是某个分区的备份副本，它必须对分区主副本的写操作进行复制，并且复制的进度不能落后太多。 如果一个备份副本挂掉、没有响应或者落后太多，主副本就会将其从同步副本集合中移除。反之，如果备份副本重新赶上主副本，它就会加入到主副本的 同步集合中。 在Kafka中，一条消息只有被ISR集合的所有副本都运用到本地的日志文件，才会认为消息被成功提交了。任何时刻，只要ISR至少有一个副本是存活的， Kafka就可以保证“一条消息一旦被提交，就不会丢失”。只有已经提交的消息才能被消费者消费，因此消费者不用担心会看到因为主副本失败而丢失的消息。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-07-13T12:05:50.012Z","updated":"2018-07-14T06:42:50.836Z","comments":true,"path":"2018/07/13/hello-world/","link":"","permalink":"https://awdclijn.github.io/2018/07/13/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}