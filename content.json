{"meta":{"title":"keon随便写写","subtitle":"大概是一些读书笔记","description":null,"author":"keon","url":"https://awdclijn.github.io"},"pages":[{"title":"categories","date":"2018-07-14T05:58:22.000Z","updated":"2018-07-14T06:07:41.138Z","comments":false,"path":"categories/index.html","permalink":"https://awdclijn.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-07-14T05:59:56.000Z","updated":"2018-07-14T06:05:49.671Z","comments":false,"path":"tags/index.html","permalink":"https://awdclijn.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"《Kafka技术内幕》--读书笔记4","slug":"《Kafka技术内幕》-读书笔记4","date":"2018-09-08T08:21:23.000Z","updated":"2018-09-11T15:30:33.339Z","comments":true,"path":"2018/09/08/《Kafka技术内幕》-读书笔记4/","link":"","permalink":"https://awdclijn.github.io/2018/09/08/《Kafka技术内幕》-读书笔记4/","excerpt":"Kafka集群的数据需要被不同类型的消费者使用，而不同类型的消费者处理逻辑不同。Kafka使用消费组的概念，允许一组消费者进程对消费工作进行划分。每个消费者都可以配置一个所属的消费组，并且订阅多个主题。Kafka会发送每条消息给每个消费组中的一个消费者迫二程(同一条消息广播给多个消费组，单播给同一组中的消费者)。被订阅主题的所有分区会平均地负载给订阅方，即消费组中的所有消费者。 Kafka采用消费组保证了“一个分区只可被消费组中的一个消费者所消费”，这意味着: 在一个消费组中，一个消费者可以消费多个分区。 不同的消费者消费的分区一定不会重复，所有消费者一起消费所有的分区。 在不同消费组中，每个消费组都会悄费所有的分区。 同一个消费组下消费者对分区是互斥的，而不同消费组之间是共享的。","text":"Kafka集群的数据需要被不同类型的消费者使用，而不同类型的消费者处理逻辑不同。Kafka使用消费组的概念，允许一组消费者进程对消费工作进行划分。每个消费者都可以配置一个所属的消费组，并且订阅多个主题。Kafka会发送每条消息给每个消费组中的一个消费者迫二程(同一条消息广播给多个消费组，单播给同一组中的消费者)。被订阅主题的所有分区会平均地负载给订阅方，即消费组中的所有消费者。 Kafka采用消费组保证了“一个分区只可被消费组中的一个消费者所消费”，这意味着: 在一个消费组中，一个消费者可以消费多个分区。 不同的消费者消费的分区一定不会重复，所有消费者一起消费所有的分区。 在不同消费组中，每个消费组都会悄费所有的分区。 同一个消费组下消费者对分区是互斥的，而不同消费组之间是共享的。 Kafka实现传统队列的方式: 发布-订阅模式。同一条消息会被多个消费组消费，每个消费组只有一个消费者，实现广播。 队列模式。只有一个消费组、多个消费者一条消息只被消费组的一个消费者消费，实现单播。 一旦出现消费组内消费者的调整，消费组内的消费者需要执行再平衡的工作。再平衡操作针对的是消费组中的所有消费者，所有消费者都妥执行重新分配分区的动作。再平衡前的消费者保存了分区的消费进度，再平衡后的消费者就可以从保存的进度位置继续读取分区。 生产者的提交日志采用递增的偏移量，连同消息内容一起写入本地日志文件。生产者客户端不需要保存偏移量相关的状态，消费者客户端则要保存消费消息的偏移盘即消费进度。消费进度表示消费者对一个分区已经消费到了哪里。 消费者对分区的消费进度通常保存在外部存储系统中，比如ZK或者Kafka的内部主题(consume_offsets)。 一个分区只能属于一个消费者线程，将分区分配给消费者有以下几种场景。 线程数量多于分区的数量，有部分钱程无法消费该主题下任何一条消息。 线程数量少于分区的数量，有一些线程会消费多个分区的数据。 线程数量等于分区的数量，则正好一个钱程消费一个分区的数据。 一个消费者线程消费多个分区，可以保证消费同一个分区的消息一定是有序的，但并不保证消费者接收到多个分区的消息完全有序。 消费者除了需要保存消费进度到ZK中，它分配的分区也是从ZK读取的。ZK不仅存储了Kafka的内部元数据，而且记录了消费组的成员列表、分区的消费进度、分区的所有者。表3-1总结了消息代理节点、主题、分区、消费者、偏移量(offset)、所有权(ownership)在ZK中的注册信息。 高级API。消费者客户端代码不需要管理偏移量的提交，并且采用了消费组的向动负载均衡功能，确保消费者的增减不会影响消息的消费。高级API提供了从Kafka消费数据的高层抽象。 低级API。通常针对特殊的消费逻辑，比如消费者只想消费某些特定的分区。低级API的客户端代码需要自己实现一些和Kafka服务端相关的底层逻辑，比如选择分区的主剧本、处理主副本的故障转移等也","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"《Kafka技术内幕》--读书笔记3","slug":"《Kafka技术内幕》-读书笔记3","date":"2018-08-13T14:54:43.000Z","updated":"2018-08-27T15:53:50.000Z","comments":true,"path":"2018/08/13/《Kafka技术内幕》-读书笔记3/","link":"","permalink":"https://awdclijn.github.io/2018/08/13/《Kafka技术内幕》-读书笔记3/","excerpt":"客户端消息发送线程我们先按照分区的主副本节点进行分组，把属于同一个节点的所有分区放在一起，合并成一个请求发送。","text":"客户端消息发送线程我们先按照分区的主副本节点进行分组，把属于同一个节点的所有分区放在一起，合并成一个请求发送。 消息被记录收集器收集，并按照分区追加到队列的最后一个批记录中。 发送钱程通过ready()从记录收集器中找出已经准备好的服务端节点。 节点已经准备好，如果客户端还没有和它们建立连接，通过connect()建立到服务端的连接。 发送线程通过drain()从记录收集器获取按照节点整理好的每个分区的批记录。 发送线程得到每个节点的批记录后，为每个节点创建客户端请求，并将请求发送到服务端。 创建生产者客户端请求发送线程并不负责真正发送客户端请求，它会从记录收集器中取出要发送的消息，创建好客户端请求，然后把请求交给客户端网络对象(NetworkClient)去发送。因为没有在发送线程中发送请求，所以创建客户端请求时需要保留目标节点，这样客户端网络对象获取出客户端请求时，才能知道要发送给哪个目标节点。 准备发送客户端请求客户端向服务端发送请求需要先建立网络连接。如果服务端还没有准备好，即还不能连接，这个节点在客户端就会被移除掉，确保消息不会发送给还没有准备好的节点;如果服务端已经准备好了，则调用selector.connect()方法建立到目标节点的网络连接。 123456789101112131415161718192021/** * Begin connecting to the given node, return true if we are already connected and ready to send to that node. * * @param node The node to check * @param now The current timestamp * @return True if we are ready to send to the given node */@Overridepublic boolean ready(Node node, long now) &#123; if (node.isEmpty()) throw new IllegalArgumentException(\"Cannot connect to empty node \" + node); if (isReady(node, now)) return true; if (connectionStates.canConnect(node.idString(), now)) // if we are interested in sending to a node and we don't have a connection to it, initiate one initiateConnect(node, now); return false;&#125; 这一步只是将请求暂存到节点对应的网络通道中，还没有真正地将客户端请求发送出去。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long now) &#123; String nodeId = clientRequest.destination(); if (!isInternalRequest) &#123; // If this request came from outside the NetworkClient, validate // that we can send data. If the request is internal, we trust // that that internal code has done this validation. Validation // will be slightly different for some internal requests (for // example, ApiVersionsRequests can be sent prior to being in // READY state.) if (!canSendRequest(nodeId)) throw new IllegalStateException(\"Attempt to send a request to node \" + nodeId + \" which is not ready.\"); &#125; AbstractRequest request = null; AbstractRequest.Builder&lt;?&gt; builder = clientRequest.requestBuilder(); try &#123; NodeApiVersions versionInfo = nodeApiVersions.get(nodeId); // Note: if versionInfo is null, we have no server version information. This would be // the case when sending the initial ApiVersionRequest which fetches the version // information itself. It is also the case when discoverBrokerVersions is set to false. if (versionInfo == null) &#123; if (discoverBrokerVersions &amp;&amp; log.isTraceEnabled()) log.trace(\"No version information found when sending message of type &#123;&#125; to node &#123;&#125;. \" + \"Assuming version &#123;&#125;.\", clientRequest.apiKey(), nodeId, builder.version()); &#125; else &#123; short version = versionInfo.usableVersion(clientRequest.apiKey()); builder.setVersion(version); &#125; // The call to build may also throw UnsupportedVersionException, if there are essential // fields that cannot be represented in the chosen version. request = builder.build(); &#125; catch (UnsupportedVersionException e) &#123; // If the version is not supported, skip sending the request over the wire. // Instead, simply add it to the local queue of aborted requests. log.debug(\"Version mismatch when attempting to send &#123;&#125; to &#123;&#125;\", clientRequest.toString(), clientRequest.destination(), e); ClientResponse clientResponse = new ClientResponse(clientRequest.makeHeader(), clientRequest.callback(), clientRequest.destination(), now, now, false, e, null); abortedSends.add(clientResponse); return; &#125; RequestHeader header = clientRequest.makeHeader(); if (log.isDebugEnabled()) &#123; int latestClientVersion = ProtoUtils.latestVersion(clientRequest.apiKey().id); if (header.apiVersion() == latestClientVersion) &#123; log.trace(\"Sending &#123;&#125; to node &#123;&#125;.\", request, nodeId); &#125; else &#123; log.debug(\"Using older server API v&#123;&#125; to send &#123;&#125; to node &#123;&#125;.\", header.apiVersion(), request, nodeId); &#125; &#125; Send send = request.toSend(nodeId, header); InFlightRequest inFlightRequest = new InFlightRequest( header, clientRequest.createdTimeMs(), clientRequest.destination(), clientRequest.callback(), clientRequest.expectResponse(), isInternalRequest, send, now); this.inFlightRequests.add(inFlightRequest); selector.send(inFlightRequest.send);&#125; 针对同一个服务端，如果上一个客户端请求还没有发送完成，则不允许发送新的客户端请求。客户端网络连接对象用inFlightRequsts变量在客户端缓存了还没有收到响应的客户端请求，InFlightRequests类包含一个节点到双端队列的映射结构。在准备发送客户端请求时，请求将添加到指定节点对应的队列中;在收到响应后，才会将请求从队列中移除。 客户端轮询并调用回调函数发送线程run()方法的最后一步是调用NetworkClient的poll()方法。轮询的最关键步骤是调用selector.poll()方法，而在轮询之后，定义了多个处理方法。轮询不仅仅会发送客户端请求，也会接收客户端响应。客户端发送请求后会调用handleCompletedSends()处理已经完成的发送，客户端接收到响应后会调用handleCompletedReceives()处理已经完成的接收。如果客户端发送完请求不需要响应，在处理已经完成的发送时，就会将对应的请求从iFlightRequests队列中移踪。而因为没有响应结果，也就不会有机会调用handleCompletedReceives()方法。如果客户端请求需要响应，则只有在handleCompletedReceives()中才会删除对应的请求:因为inFlightRequests队列保存的是未收到响应的客户端请求，请求已经有响应，就不需要存在于队列中。 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Do actual reads and writes to sockets. * * @param timeout The maximum amount of time to wait (in ms) for responses if there are none immediately, * must be non-negative. The actual timeout will be the minimum of timeout, request timeout and * metadata timeout * @param now The current time in milliseconds * @return The list of responses received */@Overridepublic List&lt;ClientResponse&gt; poll(long timeout, long now) &#123; long metadataTimeout = metadataUpdater.maybeUpdate(now); try &#123; this.selector.poll(Utils.min(timeout, metadataTimeout, requestTimeoutMs)); &#125; catch (IOException e) &#123; log.error(\"Unexpected error during I/O\", e); &#125; // process completed actions long updatedNow = this.time.milliseconds(); List&lt;ClientResponse&gt; responses = new ArrayList&lt;&gt;(); handleAbortedSends(responses); handleCompletedSends(responses, updatedNow); handleCompletedReceives(responses, updatedNow); handleDisconnections(responses, updatedNow); handleConnections(); handleInitiateApiVersionRequests(updatedNow); handleTimedOutRequests(responses, updatedNow); // invoke callbacks // 上面几个处理都会往responses中添加数据，有了响应后开始调用请求的回调函数 for (ClientResponse response : responses) &#123; try &#123; response.onComplete(); &#125; catch (Exception e) &#123; log.error(\"Uncaught error in request completion:\", e); &#125; &#125; return responses;&#125; 不需要响应的流程。开始发送请求→添加客户端请求到队列→发送请求→请求发送成功→从队列中删除发送请求→构造客户端响应。 需要晌应的流程。开始发送请求→添加客户端请求到队列→发送请求→请求发送成功→等待接收响应→接收响应→接收到完整的响应→从队列中删除客户端请求→构造客户端响应。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"《Kafka技术内幕》--读书笔记2","slug":"《Kafka技术内幕》-读书笔记2","date":"2018-07-18T14:42:29.000Z","updated":"2018-08-06T14:54:50.000Z","comments":true,"path":"2018/07/18/《Kafka技术内幕》-读书笔记2/","link":"","permalink":"https://awdclijn.github.io/2018/07/18/《Kafka技术内幕》-读书笔记2/","excerpt":"Kafka初期使用Scala编写，早期Scala版本的生产者、消费者和服务端的实现都放在core包下;而最新的客户端使用了Java重新实现，放在clients包下。 新生产者新的生产者应用程序使用KafkaProducer对象代表一个生产者客户端进程。生产者要发送消息，并不是直接发送给服务端，而是先在客户端把消息放入队列中，然后由一个消息发送线程从队列中拉取消息，以批量的方式发送消息给服务端。Kafka的记录收集器(RecordAccumulator)负责缓存生产者客户端产生的消息，发送线程(Sender)负责读取记录收集器的批量消息，通过网络发送给服务端。为了保证客户端网络请求的快速响应，Kafka使用选择器(Selector)处理网络连接和读写处理，使用网络连接(NetworkClient)处理客户端网络请求。","text":"Kafka初期使用Scala编写，早期Scala版本的生产者、消费者和服务端的实现都放在core包下;而最新的客户端使用了Java重新实现，放在clients包下。 新生产者新的生产者应用程序使用KafkaProducer对象代表一个生产者客户端进程。生产者要发送消息，并不是直接发送给服务端，而是先在客户端把消息放入队列中，然后由一个消息发送线程从队列中拉取消息，以批量的方式发送消息给服务端。Kafka的记录收集器(RecordAccumulator)负责缓存生产者客户端产生的消息，发送线程(Sender)负责读取记录收集器的批量消息，通过网络发送给服务端。为了保证客户端网络请求的快速响应，Kafka使用选择器(Selector)处理网络连接和读写处理，使用网络连接(NetworkClient)处理客户端网络请求。 发送消息Kafka源码根目录的examples包12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class Producer extends Thread &#123; private final KafkaProducer&lt;Integer, String&gt; producer; private final String topic; private final Boolean isAsync; public Producer(String topic, Boolean isAsync) &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", KafkaProperties.KAFKA_SERVER_URL + \":\" + KafkaProperties.KAFKA_SERVER_PORT); props.put(\"client.id\", \"DemoProducer\"); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.IntegerSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); producer = new KafkaProducer&lt;&gt;(props); this.topic = topic; this.isAsync = isAsync; &#125; public void run() &#123; int messageNo = 1; while (true) &#123; String messageStr = \"Message_\" + messageNo; long startTime = System.currentTimeMillis(); if (isAsync) &#123; // Send asynchronously producer.send(new ProducerRecord&lt;&gt;(topic, messageNo, messageStr), new DemoCallBack(startTime, messageNo, messageStr)); &#125; else &#123; // Send synchronously try &#123; producer.send(new ProducerRecord&lt;&gt;(topic, messageNo, messageStr)).get(); System.out.println(\"Sent message: (\" + messageNo + \", \" + messageStr + \")\"); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; ++messageNo; &#125; &#125;&#125;class DemoCallBack implements Callback &#123; private final long startTime; private final int key; private final String message; public DemoCallBack(long startTime, int key, String message) &#123; this.startTime = startTime; this.key = key; this.message = message; &#125; /** * A callback method the user can implement to provide asynchronous handling of request completion. This method will * be called when the record sent to the server has been acknowledged. Exactly one of the arguments will be * non-null. * * @param metadata The metadata for the record that was sent (i.e. the partition and offset). Null if an error * occurred. * @param exception The exception thrown during processing of this record. Null if no error occurred. */ public void onCompletion(RecordMetadata metadata, Exception exception) &#123; long elapsedTime = System.currentTimeMillis() - startTime; if (metadata != null) &#123; System.out.println( \"message(\" + key + \", \" + message + \") sent to partition(\" + metadata.partition() + \"), \" + \"offset(\" + metadata.offset() + \") in \" + elapsedTime + \" ms\"); &#125; else &#123; exception.printStackTrace(); &#125; &#125;&#125; KafkaProducer用send方法，完成同步和l异步两种模式的消息发迭。因为send方法返回的是一个Future。基于Future，我们可以实现同步或异步的消息发送语义。 同步。调用send返回Future时，需要立即调用get，因为Future.get在没有返回结果时会一直阻塞。 异步。提供一个回调，调用send后可以继续发送消息而不用等待。当有结果运回时，会向动执行回调函数。 发送消息实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * Implementation of asynchronously send a record to a topic. */private Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; TopicPartition tp = null; try &#123; // first make sure the metadata for the topic is available // 更新对应topic的元数据 ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs); long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs); Cluster cluster = clusterAndWaitTime.cluster; byte[] serializedKey; try &#123; serializedKey = keySerializer.serialize(record.topic(), record.key()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(\"Can't convert key of class \" + record.key().getClass().getName() + \" to class \" + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + \" specified in key.serializer\"); &#125; byte[] serializedValue; try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.value()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(\"Can't convert value of class \" + record.value().getClass().getName() + \" to class \" + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + \" specified in value.serializer\"); &#125; int partition = partition(record, serializedKey, serializedValue, cluster); int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue); ensureValidRecordSize(serializedSize); tp = new TopicPartition(record.topic(), partition); long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp(); log.trace(\"Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;\", record, callback, record.topic(), partition); // producer callback will make sure to call both 'callback' and interceptor callback Callback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp); RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs); if (result.batchIsFull || result.newBatchCreated) &#123; log.trace(\"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch\", record.topic(), partition); this.sender.wakeup(); &#125; return result.future; // handling exceptions and record the errors; // for API exceptions return them in the future, // for other exceptions throw directly &#125; catch (ApiException e) &#123; log.debug(\"Exception occurred during message send:\", e); if (callback != null) callback.onCompletion(null, e); this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); return new FutureFailure(e); &#125; catch (InterruptedException e) &#123; this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw new InterruptException(e); &#125; catch (BufferExhaustedException e) &#123; this.errors.record(); this.metrics.sensor(\"buffer-exhausted-records\").record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (KafkaException e) &#123; this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (Exception e) &#123; // we notify interceptor about all exceptions, since onSend is called before anything else in this method if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125;&#125; 序列化，按配置加载序列化的类12345678910111213141516if (keySerializer == null) &#123; this.keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, Serializer.class); this.keySerializer.configure(config.originals(), true);&#125; else &#123; config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG); this.keySerializer = keySerializer;&#125;if (valueSerializer == null) &#123; this.valueSerializer = config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, Serializer.class); this.valueSerializer.configure(config.originals(), false);&#125; else &#123; config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG); this.valueSerializer = valueSerializer;&#125; 计算消息要落到哪个partition12345678910111213141516171819202122232425262728/** * Compute the partition for the given record. * * @param topic The topic name * @param key The key to partition on (or null if no key) * @param keyBytes serialized key to partition on (or null if no key) * @param value The value to partition on or null * @param valueBytes serialized value to partition on or null * @param cluster The current cluster metadata */public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125;&#125; 计算消息长度是否合法12int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);ensureValidRecordSize(serializedSize); 等待批量发送消息1234567891011tp = new TopicPartition(record.topic(), partition);long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp();log.trace(\"Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;\", record, callback, record.topic(), partition);// producer callback will make sure to call both 'callback' and interceptor callbackCallback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp);RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs);if (result.batchIsFull || result.newBatchCreated) &#123; log.trace(\"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch\", record.topic(), partition); this.sender.wakeup();&#125;return result.future; 生产者发迭的消息先在客户端缓存到记录收集器RecordAccumulator中，等到一定时机再由发送线程Sender批量地写入Kafka集群。生产者每生产一条消息，就向记录收集器中追加一条消息，追加方法 的返回值表示批记录(RecordBatch)是否满了:如果批记录满了，则开始发送这一批数据。每个分区都有一个双端队列用来缓存客户端的消息，队列的每个元素是一个批记录。一旦分区的队列中有批记录满了，就会被发送线程发送到分区对应的节点;如果批记录没有满，就会继续等待直到收集到足够的消息。 追加消息时首先要获取分区所属的队列，然后取队列中最后一个批记录，如果队列中不存在批记录或者上一个批记录已经写满，应该创建新的批记录，并且加入队列的尾部。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// We keep track of the number of appending thread to make sure we do not miss batches in// abortIncompleteBatches().appendsInProgress.incrementAndGet();try &#123; // check if we have an in-progress batch Deque&lt;RecordBatch&gt; dq = getOrCreateDeque(tp); synchronized (dq) &#123; if (closed) throw new IllegalStateException(\"Cannot send after the producer is closed.\"); RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq); if (appendResult != null) return appendResult; &#125; // we don't have an in-progress record batch try to allocate a new batch int size = Math.max(this. , Records.LOG_OVERHEAD + Record.recordSize(key, value)); log.trace(\"Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;\", size, tp.topic(), tp.partition()); ByteBuffer buffer = free.allocate(size, maxTimeToBlock); synchronized (dq) &#123; // Need to check if producer is closed again after grabbing the dequeue lock. if (closed) throw new IllegalStateException(\"Cannot send after the producer is closed.\"); RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq); if (appendResult != null) &#123; // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often... free.deallocate(buffer); return appendResult; &#125; MemoryRecordsBuilder recordsBuilder = MemoryRecords.builder(buffer, compression, TimestampType.CREATE_TIME, this.batchSize); RecordBatch batch = new RecordBatch(tp, recordsBuilder, time.milliseconds()); FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, callback, time.milliseconds())); dq.addLast(batch); incomplete.add(batch); return new RecordAppendResult(future, dq.size() &gt; 1 || batch.isFull(), true); &#125;&#125; finally &#123; appendsInProgress.decrementAndGet();&#125;/** * If `RecordBatch.tryAppend` fails (i.e. the record batch is full), close its memory records to release temporary * resources (like compression streams buffers). */private RecordAppendResult tryAppend(long timestamp, byte[] key, byte[] value, Callback callback, Deque&lt;RecordBatch&gt; deque) &#123; RecordBatch last = deque.peekLast(); if (last != null) &#123; FutureRecordMetadata future = last.tryAppend(timestamp, key, value, callback, time.milliseconds()); if (future == null) last.close(); else return new RecordAppendResult(future, deque.size() &gt; 1 || last.isFull(), false); &#125; return null;&#125;","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"《Kafka技术内幕》--读书笔记1","slug":"《Kafka技术内幕》-读书笔记1","date":"2018-07-15T02:08:10.000Z","updated":"2018-07-16T13:24:38.736Z","comments":true,"path":"2018/07/15/《Kafka技术内幕》-读书笔记1/","link":"","permalink":"https://awdclijn.github.io/2018/07/15/《Kafka技术内幕》-读书笔记1/","excerpt":"基本概念 生产者 (producer)应用程序发布事件流到Kafka的一个或多个主题。 消费者 (consumer)应用程序订阅Kafka的一个或多个主题，并处理事件流。 连接器 (connector)将Kafka主题和已有数据源进行连接，数据可以互相导人和导出 。 流处理 (processor)从Kafka主题消费输入流，经过处理后，产生输出流到输出主题。","text":"基本概念 生产者 (producer)应用程序发布事件流到Kafka的一个或多个主题。 消费者 (consumer)应用程序订阅Kafka的一个或多个主题，并处理事件流。 连接器 (connector)将Kafka主题和已有数据源进行连接，数据可以互相导人和导出 。 流处理 (processor)从Kafka主题消费输入流，经过处理后，产生输出流到输出主题。 分区模型Kafka集群向多个消息代理服务器(broker server)组成，发布至UKafka集群的每条消息都有一个类别，用主题(topic)来表示。 Kafka集群为每个主题维护了分布式的分区(partition)日志文件，物理意义上可以把主题看作分区的日志文件(partitioned log)。 每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到提交日志(commit log)。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫作偏移盘(offset)，这个偏移量能够唯一地定位当前分区中的每一条消息 。 Kafka以分区作为最小的粒度，将每个分区分配给消费组中不同的而且是唯一的消费者，并确保一个分区只 属于一个消费者，即这个消费者就是这个分区的唯一读取线程 。 消费模型 Kafka采用拉取模型，由消费者向己记录消费状态，每个消费者五相独立地顺序读取每个分区的消息。生产者发布的所有消息会一直保存在Kafka集群中，不管消息有没有被消费。 用户可以通过设置保留时间来清理过期的数据。 分布式模型Kafka每个主题的多个分区日志分布式地存储在Kafka集群上，同时为了故障容错，每个分区都会以副本的方式复制到多个消息代理节点上。其中一个节点会作为主副本(Leader)，其他节点作为备份副本(Follower，也叫作从副本)。主副本会负责所有的客户端读写操作，备份副本仅仅从主副本同步数据。当主副本 出现故障时，备份副本中的一个副本会被选择为新的主副本 。 分区是消费者线程模型的最小并行单位。 其他设计持久化使用“零拷贝技术”(zero-copy)只需将磁盘文件的数据复制到页面缓存中一次，然后将数据从页面缓存直接发送到网络中(发送给不同的使用者时，都可以重复使用同一个页面缓存)，避免了重复的复制操作。 生产者与消费者Kafka的生产者将消息直接发送给分区主副本所在的消息代理节点，并不需要经过任何的中间路由层。为了做到这一点，所有消息代理节点都会保存一份相同的元数据，这份元数据记录了每个主题分区对应的主副本节点。生产者客户端在发送消息之前，会向任意一个代理节点请求元数据，井确定每条消息对应的目标节点然后把消息直接发送给对应的目标节点 。 生产者采用批量发送消息集的方式解决了网络请求过多的问题。生产者会尝试在内存中收集足够数据，并在一个请求中一次性发送一批数据。 Kafka采用了基于拉取模型的消费状态处理，它将主题分成多个有序的分区，任何时刻每个分区都只被一个消费者使用。并且，消费者会记录每个分区的消费进度(即偏移量)。Kafka的消费者会定时地将分区的消费进度保存成检查点文件，表示“这个位置之前的消息都已经被消费过了” 和生产者采用批量发送消息类似，消费者拉取消息也可以一次拉取一批消息。消费者客户端拉取消息，然后处理这一批消息，这个过程一般套在一个死循环里，表示消费者永远处于消费消息的状态 副本和容错备份副本始终尽量保持与主副本的数据同步。备份副本的日志文件和主副本的日志总是相同的，它们都有相同的偏移量和相同顺序的消息。备份副本从主副本消费消息的方式和普通的消费者一样，只不过备份副本会将消息运用到自己的本地日志文件(备份副本和主副本都在服务端，它们都会将收到的分区数据持久化成日志文件)。 kafka对节点的存活定义有两个条件: 节点必须和ZK保持会话; 如果这个节点是某个分区的备份副本，它必须对分区主副本的写操作进行复制，并且复制的进度不能落后太多。 如果一个备份副本挂掉、没有响应或者落后太多，主副本就会将其从同步副本集合中移除。反之，如果备份副本重新赶上主副本，它就会加入到主副本的 同步集合中。 在Kafka中，一条消息只有被ISR集合的所有副本都运用到本地的日志文件，才会认为消息被成功提交了。任何时刻，只要ISR至少有一个副本是存活的， Kafka就可以保证“一条消息一旦被提交，就不会丢失”。只有已经提交的消息才能被消费者消费，因此消费者不用担心会看到因为主副本失败而丢失的消息。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-07-13T12:05:50.012Z","updated":"2018-07-14T06:42:50.836Z","comments":true,"path":"2018/07/13/hello-world/","link":"","permalink":"https://awdclijn.github.io/2018/07/13/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}