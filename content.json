{"meta":{"title":"keon随便写写","subtitle":"大概是一些读书笔记","description":null,"author":"keon","url":"https://awdclijn.github.io"},"pages":[{"title":"categories","date":"2018-07-14T05:58:22.000Z","updated":"2018-07-14T06:07:41.138Z","comments":false,"path":"categories/index.html","permalink":"https://awdclijn.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-07-14T05:59:56.000Z","updated":"2018-07-14T06:05:49.671Z","comments":false,"path":"tags/index.html","permalink":"https://awdclijn.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"《Kafka技术内幕》--读书笔记2","slug":"《Kafka技术内幕》-读书笔记2","date":"2018-07-18T14:42:29.000Z","updated":"2018-08-06T14:54:50.000Z","comments":true,"path":"2018/07/18/《Kafka技术内幕》-读书笔记2/","link":"","permalink":"https://awdclijn.github.io/2018/07/18/《Kafka技术内幕》-读书笔记2/","excerpt":"Kafka初期使用Scala编写，早期Scala版本的生产者、消费者和服务端的实现都放在core包下;而最新的客户端使用了Java重新实现，放在clients包下。 新生产者新的生产者应用程序使用KafkaProducer对象代表一个生产者客户端进程。生产者要发送消息，并不是直接发送给服务端，而是先在客户端把消息放入队列中，然后由一个消息发送线程从队列中拉取消息，以批量的方式发送消息给服务端。Kafka的记录收集器(RecordAccumulator)负责缓存生产者客户端产生的消息，发送线程(Sender)负责读取记录收集器的批量消息，通过网络发送给服务端。为了保证客户端网络请求的快速响应，Kafka使用选择器(Selector)处理网络连接和读写处理，使用网络连接(NetworkClient)处理客户端网络请求。","text":"Kafka初期使用Scala编写，早期Scala版本的生产者、消费者和服务端的实现都放在core包下;而最新的客户端使用了Java重新实现，放在clients包下。 新生产者新的生产者应用程序使用KafkaProducer对象代表一个生产者客户端进程。生产者要发送消息，并不是直接发送给服务端，而是先在客户端把消息放入队列中，然后由一个消息发送线程从队列中拉取消息，以批量的方式发送消息给服务端。Kafka的记录收集器(RecordAccumulator)负责缓存生产者客户端产生的消息，发送线程(Sender)负责读取记录收集器的批量消息，通过网络发送给服务端。为了保证客户端网络请求的快速响应，Kafka使用选择器(Selector)处理网络连接和读写处理，使用网络连接(NetworkClient)处理客户端网络请求。 发送消息Kafka源码根目录的examples包12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class Producer extends Thread &#123; private final KafkaProducer&lt;Integer, String&gt; producer; private final String topic; private final Boolean isAsync; public Producer(String topic, Boolean isAsync) &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", KafkaProperties.KAFKA_SERVER_URL + \":\" + KafkaProperties.KAFKA_SERVER_PORT); props.put(\"client.id\", \"DemoProducer\"); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.IntegerSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); producer = new KafkaProducer&lt;&gt;(props); this.topic = topic; this.isAsync = isAsync; &#125; public void run() &#123; int messageNo = 1; while (true) &#123; String messageStr = \"Message_\" + messageNo; long startTime = System.currentTimeMillis(); if (isAsync) &#123; // Send asynchronously producer.send(new ProducerRecord&lt;&gt;(topic, messageNo, messageStr), new DemoCallBack(startTime, messageNo, messageStr)); &#125; else &#123; // Send synchronously try &#123; producer.send(new ProducerRecord&lt;&gt;(topic, messageNo, messageStr)).get(); System.out.println(\"Sent message: (\" + messageNo + \", \" + messageStr + \")\"); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125; &#125; ++messageNo; &#125; &#125;&#125;class DemoCallBack implements Callback &#123; private final long startTime; private final int key; private final String message; public DemoCallBack(long startTime, int key, String message) &#123; this.startTime = startTime; this.key = key; this.message = message; &#125; /** * A callback method the user can implement to provide asynchronous handling of request completion. This method will * be called when the record sent to the server has been acknowledged. Exactly one of the arguments will be * non-null. * * @param metadata The metadata for the record that was sent (i.e. the partition and offset). Null if an error * occurred. * @param exception The exception thrown during processing of this record. Null if no error occurred. */ public void onCompletion(RecordMetadata metadata, Exception exception) &#123; long elapsedTime = System.currentTimeMillis() - startTime; if (metadata != null) &#123; System.out.println( \"message(\" + key + \", \" + message + \") sent to partition(\" + metadata.partition() + \"), \" + \"offset(\" + metadata.offset() + \") in \" + elapsedTime + \" ms\"); &#125; else &#123; exception.printStackTrace(); &#125; &#125;&#125; KafkaProducer用send方法，完成同步和l异步两种模式的消息发迭。因为send方法返回的是一个Future。基于Future，我们可以实现同步或异步的消息发送语义。 同步。调用send返回Future时，需要立即调用get，因为Future.get在没有返回结果时会一直阻塞。 异步。提供一个回调，调用send后可以继续发送消息而不用等待。当有结果运回时，会向动执行回调函数。 发送消息实现：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576/** * Implementation of asynchronously send a record to a topic. */private Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) &#123; TopicPartition tp = null; try &#123; // first make sure the metadata for the topic is available // 更新对应topic的元数据 ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs); long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs); Cluster cluster = clusterAndWaitTime.cluster; byte[] serializedKey; try &#123; serializedKey = keySerializer.serialize(record.topic(), record.key()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(\"Can't convert key of class \" + record.key().getClass().getName() + \" to class \" + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() + \" specified in key.serializer\"); &#125; byte[] serializedValue; try &#123; serializedValue = valueSerializer.serialize(record.topic(), record.value()); &#125; catch (ClassCastException cce) &#123; throw new SerializationException(\"Can't convert value of class \" + record.value().getClass().getName() + \" to class \" + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() + \" specified in value.serializer\"); &#125; int partition = partition(record, serializedKey, serializedValue, cluster); int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue); ensureValidRecordSize(serializedSize); tp = new TopicPartition(record.topic(), partition); long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp(); log.trace(\"Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;\", record, callback, record.topic(), partition); // producer callback will make sure to call both 'callback' and interceptor callback Callback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp); RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs); if (result.batchIsFull || result.newBatchCreated) &#123; log.trace(\"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch\", record.topic(), partition); this.sender.wakeup(); &#125; return result.future; // handling exceptions and record the errors; // for API exceptions return them in the future, // for other exceptions throw directly &#125; catch (ApiException e) &#123; log.debug(\"Exception occurred during message send:\", e); if (callback != null) callback.onCompletion(null, e); this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); return new FutureFailure(e); &#125; catch (InterruptedException e) &#123; this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw new InterruptException(e); &#125; catch (BufferExhaustedException e) &#123; this.errors.record(); this.metrics.sensor(\"buffer-exhausted-records\").record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (KafkaException e) &#123; this.errors.record(); if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125; catch (Exception e) &#123; // we notify interceptor about all exceptions, since onSend is called before anything else in this method if (this.interceptors != null) this.interceptors.onSendError(record, tp, e); throw e; &#125;&#125; 序列化，按配置加载序列化的类12345678910111213141516if (keySerializer == null) &#123; this.keySerializer = config.getConfiguredInstance(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, Serializer.class); this.keySerializer.configure(config.originals(), true);&#125; else &#123; config.ignore(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG); this.keySerializer = keySerializer;&#125;if (valueSerializer == null) &#123; this.valueSerializer = config.getConfiguredInstance(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, Serializer.class); this.valueSerializer.configure(config.originals(), false);&#125; else &#123; config.ignore(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG); this.valueSerializer = valueSerializer;&#125; 计算消息要落到哪个partition12345678910111213141516171819202122232425262728/** * Compute the partition for the given record. * * @param topic The topic name * @param key The key to partition on (or null if no key) * @param keyBytes serialized key to partition on (or null if no key) * @param value The value to partition on or null * @param valueBytes serialized value to partition on or null * @param cluster The current cluster metadata */public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic); int numPartitions = partitions.size(); if (keyBytes == null) &#123; int nextValue = nextValue(topic); List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic); if (availablePartitions.size() &gt; 0) &#123; int part = Utils.toPositive(nextValue) % availablePartitions.size(); return availablePartitions.get(part).partition(); &#125; else &#123; // no partitions are available, give a non-available partition return Utils.toPositive(nextValue) % numPartitions; &#125; &#125; else &#123; // hash the keyBytes to choose a partition return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions; &#125;&#125; 计算消息长度是否合法12int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);ensureValidRecordSize(serializedSize); 等待批量发送消息1234567891011tp = new TopicPartition(record.topic(), partition);long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp();log.trace(\"Sending record &#123;&#125; with callback &#123;&#125; to topic &#123;&#125; partition &#123;&#125;\", record, callback, record.topic(), partition);// producer callback will make sure to call both 'callback' and interceptor callbackCallback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp);RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs);if (result.batchIsFull || result.newBatchCreated) &#123; log.trace(\"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch\", record.topic(), partition); this.sender.wakeup();&#125;return result.future; 生产者发迭的消息先在客户端缓存到记录收集器RecordAccumulator中，等到一定时机再由发送线程Sender批量地写入Kafka集群。生产者每生产一条消息，就向记录收集器中追加一条消息，追加方法 的返回值表示批记录(RecordBatch)是否满了:如果批记录满了，则开始发送这一批数据。每个分区都有一个双端队列用来缓存客户端的消息，队列的每个元素是一个批记录。一旦分区的队列中有批记录满了，就会被发送线程发送到分区对应的节点;如果批记录没有满，就会继续等待直到收集到足够的消息。 追加消息时首先要获取分区所属的队列，然后取队列中最后一个批记录，如果队列中不存在批记录或者上一个批记录已经写满，应该创建新的批记录，并且加入队列的尾部。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556// We keep track of the number of appending thread to make sure we do not miss batches in// abortIncompleteBatches().appendsInProgress.incrementAndGet();try &#123; // check if we have an in-progress batch Deque&lt;RecordBatch&gt; dq = getOrCreateDeque(tp); synchronized (dq) &#123; if (closed) throw new IllegalStateException(\"Cannot send after the producer is closed.\"); RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq); if (appendResult != null) return appendResult; &#125; // we don't have an in-progress record batch try to allocate a new batch int size = Math.max(this. , Records.LOG_OVERHEAD + Record.recordSize(key, value)); log.trace(\"Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;\", size, tp.topic(), tp.partition()); ByteBuffer buffer = free.allocate(size, maxTimeToBlock); synchronized (dq) &#123; // Need to check if producer is closed again after grabbing the dequeue lock. if (closed) throw new IllegalStateException(\"Cannot send after the producer is closed.\"); RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq); if (appendResult != null) &#123; // Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often... free.deallocate(buffer); return appendResult; &#125; MemoryRecordsBuilder recordsBuilder = MemoryRecords.builder(buffer, compression, TimestampType.CREATE_TIME, this.batchSize); RecordBatch batch = new RecordBatch(tp, recordsBuilder, time.milliseconds()); FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, callback, time.milliseconds())); dq.addLast(batch); incomplete.add(batch); return new RecordAppendResult(future, dq.size() &gt; 1 || batch.isFull(), true); &#125;&#125; finally &#123; appendsInProgress.decrementAndGet();&#125;/** * If `RecordBatch.tryAppend` fails (i.e. the record batch is full), close its memory records to release temporary * resources (like compression streams buffers). */private RecordAppendResult tryAppend(long timestamp, byte[] key, byte[] value, Callback callback, Deque&lt;RecordBatch&gt; deque) &#123; RecordBatch last = deque.peekLast(); if (last != null) &#123; FutureRecordMetadata future = last.tryAppend(timestamp, key, value, callback, time.milliseconds()); if (future == null) last.close(); else return new RecordAppendResult(future, deque.size() &gt; 1 || last.isFull(), false); &#125; return null;&#125;","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"《Kafka技术内幕》--读书笔记1","slug":"《Kafka技术内幕》-读书笔记1","date":"2018-07-15T02:08:10.000Z","updated":"2018-07-16T13:24:38.736Z","comments":true,"path":"2018/07/15/《Kafka技术内幕》-读书笔记1/","link":"","permalink":"https://awdclijn.github.io/2018/07/15/《Kafka技术内幕》-读书笔记1/","excerpt":"基本概念 生产者 (producer)应用程序发布事件流到Kafka的一个或多个主题。 消费者 (consumer)应用程序订阅Kafka的一个或多个主题，并处理事件流。 连接器 (connector)将Kafka主题和已有数据源进行连接，数据可以互相导人和导出 。 流处理 (processor)从Kafka主题消费输入流，经过处理后，产生输出流到输出主题。","text":"基本概念 生产者 (producer)应用程序发布事件流到Kafka的一个或多个主题。 消费者 (consumer)应用程序订阅Kafka的一个或多个主题，并处理事件流。 连接器 (connector)将Kafka主题和已有数据源进行连接，数据可以互相导人和导出 。 流处理 (processor)从Kafka主题消费输入流，经过处理后，产生输出流到输出主题。 分区模型Kafka集群向多个消息代理服务器(broker server)组成，发布至UKafka集群的每条消息都有一个类别，用主题(topic)来表示。 Kafka集群为每个主题维护了分布式的分区(partition)日志文件，物理意义上可以把主题看作分区的日志文件(partitioned log)。 每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到提交日志(commit log)。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫作偏移盘(offset)，这个偏移量能够唯一地定位当前分区中的每一条消息 。 Kafka以分区作为最小的粒度，将每个分区分配给消费组中不同的而且是唯一的消费者，并确保一个分区只 属于一个消费者，即这个消费者就是这个分区的唯一读取线程 。 消费模型 Kafka采用拉取模型，由消费者向己记录消费状态，每个消费者五相独立地顺序读取每个分区的消息。生产者发布的所有消息会一直保存在Kafka集群中，不管消息有没有被消费。 用户可以通过设置保留时间来清理过期的数据。 分布式模型Kafka每个主题的多个分区日志分布式地存储在Kafka集群上，同时为了故障容错，每个分区都会以副本的方式复制到多个消息代理节点上。其中一个节点会作为主副本(Leader)，其他节点作为备份副本(Follower，也叫作从副本)。主副本会负责所有的客户端读写操作，备份副本仅仅从主副本同步数据。当主副本 出现故障时，备份副本中的一个副本会被选择为新的主副本 。 分区是消费者线程模型的最小并行单位。 其他设计持久化使用“零拷贝技术”(zero-copy)只需将磁盘文件的数据复制到页面缓存中一次，然后将数据从页面缓存直接发送到网络中(发送给不同的使用者时，都可以重复使用同一个页面缓存)，避免了重复的复制操作。 生产者与消费者Kafka的生产者将消息直接发送给分区主副本所在的消息代理节点，并不需要经过任何的中间路由层。为了做到这一点，所有消息代理节点都会保存一份相同的元数据，这份元数据记录了每个主题分区对应的主副本节点。生产者客户端在发送消息之前，会向任意一个代理节点请求元数据，井确定每条消息对应的目标节点然后把消息直接发送给对应的目标节点 。 生产者采用批量发送消息集的方式解决了网络请求过多的问题。生产者会尝试在内存中收集足够数据，并在一个请求中一次性发送一批数据。 Kafka采用了基于拉取模型的消费状态处理，它将主题分成多个有序的分区，任何时刻每个分区都只被一个消费者使用。并且，消费者会记录每个分区的消费进度(即偏移量)。Kafka的消费者会定时地将分区的消费进度保存成检查点文件，表示“这个位置之前的消息都已经被消费过了” 和生产者采用批量发送消息类似，消费者拉取消息也可以一次拉取一批消息。消费者客户端拉取消息，然后处理这一批消息，这个过程一般套在一个死循环里，表示消费者永远处于消费消息的状态 副本和容错备份副本始终尽量保持与主副本的数据同步。备份副本的日志文件和主副本的日志总是相同的，它们都有相同的偏移量和相同顺序的消息。备份副本从主副本消费消息的方式和普通的消费者一样，只不过备份副本会将消息运用到自己的本地日志文件(备份副本和主副本都在服务端，它们都会将收到的分区数据持久化成日志文件)。 kafka对节点的存活定义有两个条件: 节点必须和ZK保持会话; 如果这个节点是某个分区的备份副本，它必须对分区主副本的写操作进行复制，并且复制的进度不能落后太多。 如果一个备份副本挂掉、没有响应或者落后太多，主副本就会将其从同步副本集合中移除。反之，如果备份副本重新赶上主副本，它就会加入到主副本的 同步集合中。 在Kafka中，一条消息只有被ISR集合的所有副本都运用到本地的日志文件，才会认为消息被成功提交了。任何时刻，只要ISR至少有一个副本是存活的， Kafka就可以保证“一条消息一旦被提交，就不会丢失”。只有已经提交的消息才能被消费者消费，因此消费者不用担心会看到因为主副本失败而丢失的消息。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"https://awdclijn.github.io/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"https://awdclijn.github.io/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"https://awdclijn.github.io/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"https://awdclijn.github.io/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"https://awdclijn.github.io/tags/Kafka技术内幕/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-07-13T12:05:50.012Z","updated":"2018-07-14T06:42:50.836Z","comments":true,"path":"2018/07/13/hello-world/","link":"","permalink":"https://awdclijn.github.io/2018/07/13/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}