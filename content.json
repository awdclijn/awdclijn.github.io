{"meta":{"title":"keon随便写写","subtitle":"大概是一些读书笔记","description":null,"author":"keon","url":"http://yoursite.com"},"pages":[{"title":"categories","date":"2018-07-14T05:58:22.000Z","updated":"2018-07-14T06:07:41.138Z","comments":false,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2018-07-14T05:59:56.000Z","updated":"2018-07-14T06:05:49.671Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"《Kafka技术内幕》--读书笔记1","slug":"《Kafka技术内幕》-读书笔记1","date":"2018-07-15T02:08:10.000Z","updated":"2018-07-16T13:24:38.736Z","comments":true,"path":"2018/07/15/《Kafka技术内幕》-读书笔记1/","link":"","permalink":"http://yoursite.com/2018/07/15/《Kafka技术内幕》-读书笔记1/","excerpt":"基本概念 生产者 (producer)应用程序发布事件流到Kafka的一个或多个主题。 消费者 (consumer)应用程序订阅Kafka的一个或多个主题，并处理事件流。 连接器 (connector)将Kafka主题和已有数据源进行连接，数据可以互相导人和导出 。 流处理 (processor)从Kafka主题消费输入流，经过处理后，产生输出流到输出主题。","text":"基本概念 生产者 (producer)应用程序发布事件流到Kafka的一个或多个主题。 消费者 (consumer)应用程序订阅Kafka的一个或多个主题，并处理事件流。 连接器 (connector)将Kafka主题和已有数据源进行连接，数据可以互相导人和导出 。 流处理 (processor)从Kafka主题消费输入流，经过处理后，产生输出流到输出主题。 分区模型Kafka集群向多个消息代理服务器(broker server)组成，发布至UKafka集群的每条消息都有一个类别，用主题(topic)来表示。 Kafka集群为每个主题维护了分布式的分区(partition)日志文件，物理意义上可以把主题看作分区的日志文件(partitioned log)。 每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到提交日志(commit log)。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号，叫作偏移盘(offset)，这个偏移量能够唯一地定位当前分区中的每一条消息 。 Kafka以分区作为最小的粒度，将每个分区分配给消费组中不同的而且是唯一的消费者，并确保一个分区只 属于一个消费者，即这个消费者就是这个分区的唯一读取线程 。 消费模型 Kafka采用拉取模型，由消费者向己记录消费状态，每个消费者五相独立地顺序读取每个分区的消息。生产者发布的所有消息会一直保存在Kafka集群中，不管消息有没有被消费。 用户可以通过设置保留时间来清理过期的数据。 分布式模型Kafka每个主题的多个分区日志分布式地存储在Kafka集群上，同时为了故障容错，每个分区都会以副本的方式复制到多个消息代理节点上。其中一个节点会作为主副本(Leader)，其他节点作为备份副本(Follower，也叫作从副本)。主副本会负责所有的客户端读写操作，备份副本仅仅从主副本同步数据。当主副本 出现故障时，备份副本中的一个副本会被选择为新的主副本 。 分区是消费者线程模型的最小并行单位。 其他设计持久化使用“零拷贝技术”(zero-copy)只需将磁盘文件的数据复制到页面缓存中一次，然后将数据从页面缓存直接发送到网络中(发送给不同的使用者时，都可以重复使用同一个页面缓存)，避免了重复的复制操作。 生产者与消费者Kafka的生产者将消息直接发送给分区主副本所在的消息代理节点，并不需要经过任何的中间路由层。为了做到这一点，所有消息代理节点都会保存一份相同的元数据，这份元数据记录了每个主题分区对应的主副本节点。生产者客户端在发送消息之前，会向任意一个代理节点请求元数据，井确定每条消息对应的目标节点然后把消息直接发送给对应的目标节点 。 生产者采用批量发送消息集的方式解决了网络请求过多的问题。生产者会尝试在内存中收集足够数据，并在一个请求中一次性发送一批数据。 Kafka采用了基于拉取模型的消费状态处理，它将主题分成多个有序的分区，任何时刻每个分区都只被一个消费者使用。并且，消费者会记录每个分区的消费进度(即偏移量)。Kafka的消费者会定时地将分区的消费进度保存成检查点文件，表示“这个位置之前的消息都已经被消费过了” 和生产者采用批量发送消息类似，消费者拉取消息也可以一次拉取一批消息。消费者客户端拉取消息，然后处理这一批消息，这个过程一般套在一个死循环里，表示消费者永远处于消费消息的状态 副本和容错备份副本始终尽量保持与主副本的数据同步。备份副本的日志文件和主副本的日志总是相同的，它们都有相同的偏移量和相同顺序的消息。备份副本从主副本消费消息的方式和普通的消费者一样，只不过备份副本会将消息运用到自己的本地日志文件(备份副本和主副本都在服务端，它们都会将收到的分区数据持久化成日志文件)。 kafka对节点的存活定义有两个条件: 节点必须和ZK保持会话; 如果这个节点是某个分区的备份副本，它必须对分区主副本的写操作进行复制，并且复制的进度不能落后太多。 如果一个备份副本挂掉、没有响应或者落后太多，主副本就会将其从同步副本集合中移除。反之，如果备份副本重新赶上主副本，它就会加入到主副本的 同步集合中。 在Kafka中，一条消息只有被ISR集合的所有副本都运用到本地的日志文件，才会认为消息被成功提交了。任何时刻，只要ISR至少有一个副本是存活的， Kafka就可以保证“一条消息一旦被提交，就不会丢失”。只有已经提交的消息才能被消费者消费，因此消费者不用担心会看到因为主副本失败而丢失的消息。","categories":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://yoursite.com/categories/读书笔记/"},{"name":"Kafka技术内幕","slug":"读书笔记/Kafka技术内幕","permalink":"http://yoursite.com/categories/读书笔记/Kafka技术内幕/"}],"tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"},{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"},{"name":"Kafka技术内幕","slug":"Kafka技术内幕","permalink":"http://yoursite.com/tags/Kafka技术内幕/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-07-13T12:05:50.012Z","updated":"2018-07-14T06:42:50.836Z","comments":true,"path":"2018/07/13/hello-world/","link":"","permalink":"http://yoursite.com/2018/07/13/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}